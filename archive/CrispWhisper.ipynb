{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file transcript test\n",
    "audio_path = '../data/'\n",
    "transcript_path = '../data/transcripts/'\n",
    "audio_file = '../data/audiotest.wav'\n",
    "#audio_file = '../data/penelope.mp3'\n",
    "\n",
    "\n",
    "\n",
    "audio, sr = librosa.load(audio_file, sr=16000)\n",
    "ipd.Audio(audio, rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Audio\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "def adjust_pauses_for_hf_pipeline_output(pipeline_output, split_threshold=0.12):\n",
    "    \"\"\"\n",
    "    Adjust pause timings by distributing pauses up to the threshold evenly between adjacent words.\n",
    "    \"\"\"\n",
    "\n",
    "    adjusted_chunks = pipeline_output[\"chunks\"].copy()\n",
    "\n",
    "    for i in range(len(adjusted_chunks) - 1):\n",
    "        current_chunk = adjusted_chunks[i]\n",
    "        next_chunk = adjusted_chunks[i + 1]\n",
    "\n",
    "        current_start, current_end = current_chunk[\"timestamp\"]\n",
    "        next_start, next_end = next_chunk[\"timestamp\"]\n",
    "        pause_duration = next_start - current_end\n",
    "\n",
    "        if pause_duration > 0:\n",
    "            if pause_duration > split_threshold:\n",
    "                distribute = split_threshold / 2\n",
    "            else:\n",
    "                distribute = pause_duration / 2\n",
    "\n",
    "            # Adjust current chunk end time\n",
    "            adjusted_chunks[i][\"timestamp\"] = (current_start, current_end + distribute)\n",
    "\n",
    "            # Adjust next chunk start time\n",
    "            adjusted_chunks[i + 1][\"timestamp\"] = (next_start - distribute, next_end)\n",
    "    pipeline_output[\"chunks\"] = adjusted_chunks\n",
    "\n",
    "    return pipeline_output\n",
    "\n",
    "# remember to open the \"venv\" (source venv/bin/activate) then (where required) \n",
    "# basic requirements\n",
    "#    pip install torch torchaudio transformers datasets accelerate\n",
    "# I think torch has buch of native cuda stuff for NVIDIA cards which I have not had for years\n",
    "\n",
    "############## # For Integrated GPUs\n",
    "#device = \"cpu\"\n",
    "#torch_dtype = torch.float32\n",
    "##############\n",
    "\n",
    "############## # For NVIDIA GPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "#############\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Request access to the model at nyrahealth/CrisperWhisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import login\n",
    "#login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# initialise model\n",
    "model_id = \"nyrahealth/CrisperWhisper\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    "#    use_cache=True, # there seems to be a common issue with the AutoModelForSpeechSeq2Seq model, \n",
    "                     # where the caching size grows indefinitely, causing a \"CUDA out of memory\" error.\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# gradient checkpointing can reduce the memory requirements during training, \n",
    "# allowing you to train larger models or use larger batch sizes\n",
    "#model.gradient_checkpointing_enable()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",                 # Type of task\n",
    "    model=model,                                    # The loaded CrisperWhisper model\n",
    "    tokenizer=processor.tokenizer,                  # Converts text to tokens\n",
    "    feature_extractor=processor.feature_extractor,  # Processes audio input\n",
    "    chunk_length_s=30,                              # Process audio in 30-second chunks\n",
    "    batch_size=16,                                   # Number of chunks processed at once\n",
    "    return_timestamps='word',                       # Get word-level timestamps\n",
    "    torch_dtype=torch_dtype,                        # Precision (float16 for GPU, float32 for CPU)\n",
    "    device=device,                                  # Computing device (GPU/CPU)\n",
    "    model_kwargs={\"language\": \"en\"}\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline_output = pipe(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_data = adjust_pauses_for_hf_pipeline_output(hf_pipeline_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcription_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def plot_interactive_waveform(audio_path, transcription_data):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Create time array\n",
    "    time = np.arange(len(y)) / sr\n",
    "\n",
    "    # Create figure\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "    # Add waveform\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time, y=y, name='Waveform'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add word segments\n",
    "    for chunk in transcription_data['chunks']:\n",
    "        start_time, end_time = chunk['timestamp']\n",
    "        text = chunk['text']\n",
    "\n",
    "        # Add segment highlight\n",
    "        fig.add_vrect(\n",
    "            x0=start_time,\n",
    "            x1=end_time,\n",
    "            fillcolor=\"rgba(0,0,255,0.1)\",\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        # Add text annotation\n",
    "        fig.add_annotation(\n",
    "            x=start_time,\n",
    "            y=0,\n",
    "            text=text,\n",
    "            showarrow=False,\n",
    "            textangle=45,\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Interactive Waveform with Word-Level Annotations',\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Usage:\n",
    "# plot_interactive_waveform('your_audio_file.wav', transcription_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interactive_waveform(audio_file, transcription_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_transcription(transcription_data, transcript_path, file_identifier=None, format_type=\"text_only\"):\n",
    "    \"\"\"\n",
    "    Save transcription data to the specified path\n",
    "\n",
    "    Args:\n",
    "        transcription_data: The CrisperWhisper output dictionary\n",
    "        transcript_path: Directory path to save transcripts\n",
    "        file_identifier: Identifier for the file (audio filename or timestamp)\n",
    "        format_type: \"text_only\", \"timestamped\", or \"json\"\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(transcript_path, exist_ok=True)\n",
    "\n",
    "    # Generate identifier if not provided\n",
    "    if file_identifier is None:\n",
    "        file_identifier = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create full output path with appropriate extension\n",
    "    extension = \".json\" if format_type == \"json\" else \".txt\"\n",
    "    filename = f\"{file_identifier}_{format_type}{extension}\"\n",
    "    output_file_path = os.path.join(transcript_path, filename)\n",
    "\n",
    "    if format_type == \"text_only\":\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(transcription_data[\"text\"])\n",
    "\n",
    "    elif format_type == \"timestamped\":\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            for chunk in transcription_data[\"chunks\"]:\n",
    "                start, end = chunk[\"timestamp\"]\n",
    "                end_str = f\"{end:.2f}\" if end is not None else \"END\"\n",
    "                f.write(f\"[{start:.2f}-{end_str}] {chunk['text']}\\n\")\n",
    "\n",
    "    elif format_type == \"json\":\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(transcription_data, f, indent=2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"format_type must be 'text_only', 'timestamped', or 'json'\")\n",
    "\n",
    "    return output_file_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the audio file is named \"interview1.wav\"\n",
    "output_file = save_transcription(transcription_data, \n",
    "                                transcript_path,\n",
    "                                file_identifier=\"audiotest\", \n",
    "                                format_type=\"json\") # format_type: \"text_only\", \"timestamped\", or \"json\"\n",
    "print(f\"Saved transcript to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
