{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632222b7-0206-474f-b0ba-f8e14561dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "\n",
    "# remember to open the \"venv\" (source venv/bin/activate) then (where required) \n",
    "# basic requirements\n",
    "#    pip install torch torchaudio transformers datasets accelerate\n",
    "# I think torch has buch of native cuda stuff for NVIDIA cards which I have not had for years\n",
    "\n",
    "############## # For Integrated GPUs\n",
    "device = \"cpu\"\n",
    "torch_dtype = torch.float32\n",
    "##############\n",
    "\n",
    "############## # For NVIDIA GPU\n",
    "#device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d958c0-66c3-49cd-9cf2-7cd99b6a83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c200b80-e4a2-4682-b57a-a6c00d9049ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_pauses_for_hf_pipeline_output(pipeline_output, split_threshold=0.12):\n",
    "    \"\"\"\n",
    "    Adjust pause timings by distributing pauses up to the threshold evenly between adjacent words.\n",
    "    \"\"\"\n",
    "\n",
    "    adjusted_chunks = pipeline_output[\"chunks\"].copy()\n",
    "\n",
    "    for i in range(len(adjusted_chunks) - 1):\n",
    "        current_chunk = adjusted_chunks[i]\n",
    "        next_chunk = adjusted_chunks[i + 1]\n",
    "\n",
    "        current_start, current_end = current_chunk[\"timestamp\"]\n",
    "        next_start, next_end = next_chunk[\"timestamp\"]\n",
    "        pause_duration = next_start - current_end\n",
    "\n",
    "        if pause_duration > 0:\n",
    "            if pause_duration > split_threshold:\n",
    "                distribute = split_threshold / 2\n",
    "            else:\n",
    "                distribute = pause_duration / 2\n",
    "\n",
    "            # Adjust current chunk end time\n",
    "            adjusted_chunks[i][\"timestamp\"] = (current_start, current_end + distribute)\n",
    "\n",
    "            # Adjust next chunk start time\n",
    "            adjusted_chunks[i + 1][\"timestamp\"] = (next_start - distribute, next_end)\n",
    "    pipeline_output[\"chunks\"] = adjusted_chunks\n",
    "\n",
    "    return pipeline_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f5f91-de1a-451e-951c-7d3536388da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Request access to the model at nyrahealth/CrisperWhisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aaf999-f3c2-48fa-8d74-a3a03f45004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import login\n",
    "#login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a6a2e-a0f2-4b95-8b2f-b4c593c7557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise model\n",
    "model_id = \"nyrahealth/CrisperWhisper\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79249e13-4559-4541-b85c-d6fc0003e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d250af1-1e4a-401a-a5d3-e98d38997b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", # Type of task\n",
    "    model=model,                    # The loaded CrisperWhisper model\n",
    "    tokenizer=processor.tokenizer,  # Converts text to tokens\n",
    "    feature_extractor=processor.feature_extractor, # Processes audio input\n",
    "    chunk_length_s=30,              # Process audio in 30-second chunks\n",
    "    batch_size=16,                  # Number of chunks processed at once\n",
    "    return_timestamps='word',       # Get word-level timestamps\n",
    "    torch_dtype=torch_dtype,        # Precision (float16 for GPU, float32 for CPU)\n",
    "    device=device,                  # Computing device (GPU/CPU)\n",
    "    model_kwargs={\"language\": \"en\"}\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168d79b-7a9a-4605-8edb-94a26dac3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file transcript test\n",
    "audio_path = '/home/diego/CRIISP-WP6/data/'\n",
    "transcript_path = '/home/diego/CRIISP-WP6/data/transcripts/'\n",
    "audio_file = '/home/diego/CRIISP-WP6/data/audiotest.wav'\n",
    "Audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d9447-4747-42a5-938d-3ea146af1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline_output = pipe(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3446803-9c92-4ffc-ab09-8262a9925580",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_data = adjust_pauses_for_hf_pipeline_output(hf_pipeline_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e913635-8b7f-4ec1-8d84-cdbd88b20311",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcription_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc65db3-d17b-4c03-bdf9-e122378ea820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def plot_interactive_waveform(audio_path, transcription_data):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Create time array\n",
    "    time = np.arange(len(y)) / sr\n",
    "\n",
    "    # Create figure\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "    # Add waveform\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time, y=y, name='Waveform'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add word segments\n",
    "    for chunk in transcription_data['chunks']:\n",
    "        start_time, end_time = chunk['timestamp']\n",
    "        text = chunk['text']\n",
    "\n",
    "        # Add segment highlight\n",
    "        fig.add_vrect(\n",
    "            x0=start_time,\n",
    "            x1=end_time,\n",
    "            fillcolor=\"rgba(0,0,255,0.1)\",\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        # Add text annotation\n",
    "        fig.add_annotation(\n",
    "            x=start_time,\n",
    "            y=0,\n",
    "            text=text,\n",
    "            showarrow=False,\n",
    "            textangle=45,\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Interactive Waveform with Word-Level Annotations',\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Usage:\n",
    "# plot_interactive_waveform('your_audio_file.wav', transcription_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cb746-5f53-4c66-858d-b468b28ed3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0779fd-c289-4afa-bc75-b99b4a8ba744",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interactive_waveform(audio_file, transcription_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0eea23-eac3-41bf-bd55-54566b2b2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_transcription(transcription_data, transcript_path, file_identifier=None, format_type=\"text_only\"):\n",
    "    \"\"\"\n",
    "    Save transcription data to the specified path\n",
    "\n",
    "    Args:\n",
    "        transcription_data: The CrisperWhisper output dictionary\n",
    "        transcript_path: Directory path to save transcripts\n",
    "        file_identifier: Identifier for the file (audio filename or timestamp)\n",
    "        format_type: \"text_only\", \"timestamped\", or \"json\"\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(transcript_path, exist_ok=True)\n",
    "\n",
    "    # Generate identifier if not provided\n",
    "    if file_identifier is None:\n",
    "        file_identifier = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create full output path with appropriate extension\n",
    "    extension = \".json\" if format_type == \"json\" else \".txt\"\n",
    "    filename = f\"{file_identifier}_{format_type}{extension}\"\n",
    "    output_file_path = os.path.join(transcript_path, filename)\n",
    "\n",
    "    if format_type == \"text_only\":\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(transcription_data[\"text\"])\n",
    "\n",
    "    elif format_type == \"timestamped\":\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            for chunk in transcription_data[\"chunks\"]:\n",
    "                start, end = chunk[\"timestamp\"]\n",
    "                end_str = f\"{end:.2f}\" if end is not None else \"END\"\n",
    "                f.write(f\"[{start:.2f}-{end_str}] {chunk['text']}\\n\")\n",
    "\n",
    "    elif format_type == \"json\":\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(transcription_data, f, indent=2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"format_type must be 'text_only', 'timestamped', or 'json'\")\n",
    "\n",
    "    return output_file_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae2dc5-eb1a-457e-a0e2-ea067d41d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the audio file is named \"interview1.wav\"\n",
    "output_file = save_transcription(transcription_data, \n",
    "                                '/home/diego/CRIISP-WP6/data/transcripts/',\n",
    "                                file_identifier=\"audiotest\", \n",
    "                                format_type=\"json\") # format_type: \"text_only\", \"timestamped\", or \"json\"\n",
    "print(f\"Saved transcript to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
