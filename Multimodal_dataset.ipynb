{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Recognition Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "**Aim** to create a multimodal emotion recognition dataset from natural speech collected in naturalistic enviroment. The dataset will integrate two modalities: voice and corresponding transcripts\n",
    "\n",
    "## Introduction\n",
    "The audio and text-based emotion encoder proposed follows and expands the work by Liampis, Karanikola, and Kotsiantis ([2025](https://www.sciencedirect.com/science/article/pii/S0925231225004941)) who model the serial, distributional structure of emotions in text by converting sequences of sentences into sequences of emotion labels, and then generating distributional emotion embeddings from these sequences. The idea behind their work is that emotions in text are not isolated states but unfold as structured, serially-dependent patterns akin to a time series and forming a sequence that reflects the emotional evolution throughout the discourse. The authors argue that the emotional states exhibit distributional patterns as they appear and alternate in text, very much like semantic relationships between words are shaped by their order and co-occurrence. In our multimodal pipeline, we extend this idea with an audio modality: we extract high-level emotion embeddings from each segment using emotion2vec which capture contextual acoustic emotion features. We then use timestamps to align and concatenate these representations at the segment level to form a multimodal feature space.  \n",
    "\n",
    "\n",
    "### Conceptual Framework\n",
    "\n",
    "#### Text component\n",
    "* *Serial Emotional Footprint in Text*: Building on Liapis, Karanikola, and Kotsiantis (2025), we treat the emotional flow in natural language as a structured, serially-dependent pattern. Each segment of text (e.g., sentence or utterance) is assigned a set of emotion labels using a multi-label classifier. These labels are then arranged in order, forming an \"emotion string\" that reflects the temporal progression of emotions throughout the transcript. This sequence is modeled as a time-series-like structure, capturing the distributional regularities and interdependencies of emotions as they appear and alternate in the text\n",
    "* *Distributional Emotion Embeddings for Text*: The emotion label sequences (emotion strings) derived from the text are embedded using distributional models (e.g., Word2Vec trained on the emotion label corpus) and/or contextual transformer-based models. This process encodes both the semantic and sequential properties of emotional expression in the text, allowing downstream models to leverage latent information about emotion transitions and dependencies that cannot be captured by isolated emotion labels alone\n",
    "#### Audio Component\n",
    "* *Contextual Acoustic Emotion Embeddings for Audio*: For the audio modality, we extract high-level, segment-level emotion embeddings using a state-of-the-art model (e.g. emotion2vec). These embeddings capture the contextual acoustic features relevant to emotional expression in speech but are not modeled as distributional or sequential emotion label strings. Instead, each audio segment (the audio stream corresponding to a phrasal unit) is represented by its corresponding emotion2vec embedding, which encodes paralinguistic cues and affective information from the speech signal.\n",
    "\n",
    "#### Multimodal integration\n",
    "* *Multimodal Alignment and Fusion*: Audio and text segments are aligned using precise timestamps obtained from the transcription process (via crisper-whisper). For each aligned segment, the contextual acoustic embedding (from audio) and the distributional emotion embedding (from text) are concatenated to form a multimodal feature vector. This approach should integrate the complementary strengths of both modalities: the structured, sequential modeling of emotions in text and the rich paralinguistic information in speech. By aligning and fusing these representations at the segment level, we create a comprehensive multimodal feature space for emotion recognition, suitable for robust analysis in naturalistic, diary-style speech data.\n",
    "\n",
    "## Dataset Structure\n",
    "Two data modes:\n",
    "* **Audio**: segment-level emotion embeddings and labels.\n",
    "* **Text**: Tokenized transcripts, segment-level emotion embeddings and labels.\n",
    "\n",
    "### Data Collection\n",
    "\n",
    "* **Speech Recordings**: High-quality (16KhZ), natural (and naturalistic) speech samples. \n",
    "* **Text Transcripts**: Time-aligned transcripts for each audio recording (synchronization at the utterance level.\n",
    "* **Emotion Annotation**: The present pipeline is using only automatic emotion classifiers (e.g., fine-tuned transformers, emotion2vec). In addition we will need to add human annotators to label each segment with one or more emotion categories.\n",
    "\n",
    "\n",
    "## Method\n",
    "\n",
    "### Tools\n",
    "\n",
    "* [emotion2vec: audio-based emotion embeddings](https://huggingface.co/emotion2vec/emotion2vec_plus_large)\n",
    "* [emotion-english-distilroberta-base](https://huggingface.co/j-hartmann/emotion-english-roberta-large)\n",
    "* [CrisperWhisper](https://github.com/nyrahealth/CrisperWhisper)\n",
    "\n",
    "**Emotion2vec** is the first universal speech emotion representation model. Through self-supervised pre-training, emotion2vec has the ability to extract emotion representation across different tasks, languages, and scenarios. Emotion2vec aims to create a universal emotional representation space. \n",
    "**Emotion-english-distilroberta-base** model is a distilled version of RoBERTa-large trained on 6 diverse datasets and predicts Ekman's 6 basic emotions, plus a neutral class. RoBERTa-large (Liu et al. 2019) is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. \n",
    "**Crisper-whisper** is an advanced variant of OpenAI's Whisper, designed for fast, precise, and verbatim speech recognition with accurate (crisp) word-level timestamps. Unlike the original Whisper, which tends to omit disfluencies and follows more of a intended transcription style, CrisperWhisper aims to transcribe every spoken word exactly as it is, including fillers, pauses, stutters and false starts.\n",
    "**Text Anonymization Pipeline**: For automated and manual redaction of personal identifiers in transcripts with Presidio (To be done). Presidio is an open-source NLP library developed by Microsoft, designed for automated text redaction and anonymization\n",
    "\n",
    "### Data\n",
    "\n",
    "| Item             | Description                                                             | Included in Dataset  | Requires Anonymization |\n",
    "|------------------|-------------------------------------------------------------------------|----------------------|------------------------|\n",
    "| Audio File       | A .wav file containing speech (e.g., audiotest.wav)                     | No                   | No                     |\n",
    "| Audio embeddings | Extracted, non-reconstructable, privacy-checked embeddings              | Yes                  | No                     |\n",
    "| Transcripts      | Output of crisperwhisper (JSON format) with utterance-level timestamps  | Yes                  | Yes                    |\n",
    "| Timestamps       | Used for alignment, included in metadata                                | Yes                  | No                     |\n",
    "\n",
    "\n",
    "### Multimodal Processing\n",
    "\n",
    "#### Step 0. Transcript generation\n",
    "\n",
    "* Transcribe the Audio: Convert the audio file into text using Crisperwhisper to obtain transcripts with word-level timestamps.\n",
    "    * Anonymisation: we use Microsoft Presidio (automated tool) - **This needs testing, not implemented yet**\n",
    "\n",
    "\n",
    "#### Step 1. Text embeddings extraction (mode 1)\n",
    "This is the process to generate distributional embeddings [Liampis 2025](https://www.sciencedirect.com/science/article/pii/S0925231225004941).\n",
    "\n",
    "Two different representations are needed for the final embedding to be constructed. \n",
    "\n",
    "1. The first representation concerns an (ideally extensive) textual corpus that is called *base corpus*, from which the base embeddings are generated (and/or) extracted.\n",
    "2. The second representation concerns the data one wants to model or *target data*.\n",
    "\n",
    "First we extract (and/or) generate our base embeddings from the base corpus (step 1.1) and then we generate distributional embeddings (2.2).\n",
    "\n",
    "##### Step 1.1 Embedding generation/extraction (from the base corpus)\n",
    "\n",
    "![Emotion string sequence](https://ars.els-cdn.com/content/image/1-s2.0-S0925231225004941-fx1001.jpg)\n",
    "\n",
    "Followin this algorithm we start by (1) tokenizing the base corpus into sentences and (2) then we extract, through the use of transformer-based multi-label emotion classification, the corresponding emotions for each sentence. In this case point (2) implies the assumption that the output of the emotion classifier is the groud-truth. The authors suggest that it would be ideal to have pre-labeled base corpus instead of using a transformer classifier to allocate these labels. (1) and (2) are conducted as follow:\n",
    "\n",
    "1. segment tokenisation: segment the transcript into sentences - via nltk (a natural language tokenizer).\n",
    "    1. allocate timestamps to each sentence (precise start and stop timing of each segment), this will preserve alignment with between audio and txt segments for later on.\n",
    "2. For each segment:\n",
    "    1.  we apply the [Emotion English DistilRoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large)  model to obtain Hartman's  emotion probabilities (or any other suitable multi-label emotion classifier).\n",
    "    2. we serialise the top-$k$ (k=3) emotion labels of each segment into a new txt string Emotion1__Emotion2__Emotion3 (e.g.\"joy__surprise__fear\") in order of their probability value. We call this token **emotion string** and the vocolabulary of this token is as \"large\" as $k$ - larger k will involve more subtle representation of the underlying true emotion.\n",
    "3. We concate the emotion strings in the temporal order of appearance to create a time-series-like structure (**base emotion corpus**), e.g.:`Sentence 1: Anger__Joy__Trust; Sentence 2: \"Frustration__neutral__anger\"; ... `. This corpus organises emotion strings as tokens in a \"text\" where the vocabulary consists only of emotion labels. \n",
    "\n",
    "Thus, while each *emotion string* captures the emotion dominance (top-k probability value) of each segment, the concatenation of these emotion strings to forms the *base emotion corpus* captures the temporal sequence of those emotions strings. This is an important distinction when we decide whether generating or extracting (or both) base embeddings.\n",
    "\n",
    "4. Base embedding extraction/generation:\n",
    "    - Option 1: Extraction (Contextual Embeddings): Use a pretrained sentence transformer (e.g., all-MiniLM-L6-v2) to embed emotion strings, capturing semantic relationships between them: the embeddings will be a representation of the emotion combinations and _do not_ include any serial (temporal) interdependencies between the emotion label tokens (i.e. emotion strings).\n",
    "    - Option 2: Generation (Distributional Embeddings): Train a Word2Vec model on the emotion string sequences to encode sequential dependencies (e.g., how \"Joy\" often precedes \"Surprise\").\n",
    "\n",
    "Those can both carried out and combined in the step 1.2 Distributional embedding generation:\n",
    "\n",
    "##### Step 1.2 Distributional Embeddings (from the base corpus)\n",
    "\n",
    "\n",
    "\n",
    "![Distributional embeddings](https://ars.els-cdn.com/content/image/1-s2.0-S0925231225004941-gr5.jpg)\n",
    "\n",
    "\n",
    "\n",
    "#### Step 2. Audio embeddings extraction (mode 2)\n",
    "\n",
    "We use emotion2vec to extract high-level emotion embeddings from the audio file.\n",
    "\n",
    "Note: We only publish these embeddings, not the raw audio. We will also look into further processing of the embeddings to minimize speaker-identifiable information (using e.g. dimensionality reduction via umap).\n",
    "\n",
    "\n",
    "#### Step 3. Multimodal Integration\n",
    "\n",
    "1. Align audio and text segments using timestamps.\n",
    "    * Segment Matching: Map audio segments (from emotion2vec) to text segments using timestamps (e.g., a 3-second audio chunk and its corresponding transcribed sentence).\n",
    "    * Embedding Concatenation: For each aligned segment, concatenate:\n",
    "        * Audio: Emotion2vec embedding (contextual acoustic features).\n",
    "        * Text: Distributional emotion embedding (serial emotion patterns).\n",
    "\n",
    "As we concatenate the audio emotion embedding and the corresponding text-based emotion embedding we form a multimodal feature vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from funasr import AutoModel\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Multimodal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step.0 transcript generation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step.1  Text Embedding Extraction/generation\n",
    "\n",
    "In the previous paragraph we tokenized the base corpus into sentences (via `segment_into_sentences()`). Now, [the paper](https://www.sciencedirect.com/science/article/pii/S0925231225004941) suggests that: \n",
    "\n",
    "1. we use a pre-trained transformer to generate a dataset of labelled sentences *and*\n",
    "2. we assume that the output of the classifier is our ground truth in terms of assigning emotion labels to sentences.\n",
    "\n",
    "\n",
    "\n",
    "The textual sequence (our transcript) will have a corresponding emotion-based vector representation\n",
    "\n",
    "We define the function `classify_emotions()` in 2 steps:\n",
    "\n",
    "### Emotion classification\n",
    "* Step 1: we extract the corresponding emotions for each sentence using the pre-trained transformer ([Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base)) to get the corresponding emotions for each sentence. This approach will lead to having an array of emotions with their probability value. \n",
    "\n",
    "### Create Emotion Label Texts\n",
    "* Step 2: We take the output of `classify_emotions()` and sentence by sentence we interpret the probability value of each emotions in terms of emotion dominance and arrange their corresponding textual labels, i.e., as if they were strings, in serially ordered layout. Very much as if the labels were parts of a text. Therefore, for each sentence in the transcript we find the three most predominant emotions by probability (what the paper calls *\"sort extracted emotions by their probability values\"*). Subsequently, for each sentence `classify_emotions()` generates \"emotion_sequences\" txt by joining (in a serial manner) the top 3 emotions with \"__\" separators. This step implements what [the paper](https://www.sciencedirect.com/science/article/pii/S0925231225004941) calls *\"arrange emotion labels in a serially ordered layout as if the labels were parts of a text. The vocabulary of such a text is the emotion labels.\"\n",
    "\n",
    "\n",
    "#### method A: contextual Embedding Generation\n",
    "For each emotion string, it uses a transformer model (here, distilbert-base-uncased) to generate a contextual embedding for the sequence (using the [CLS] token as the representation). The result is a set of contextualized vector representations for the serial emotion strings.\n",
    "The final  output is a sequence of embeddings, one per sentence/utterance, each representing the dominant emotions and their context in the sequence.\n",
    "#### method B: distributional Embedding Generation\n",
    "\n",
    "According to the paper the Distributional Representations requires to:\n",
    "\n",
    "1. *\"treat each sentence as an observation in a multivariate series of emotions\"* \n",
    "2. *\"transform the emotional flow of a text into a sequence of emotion strings.\"*\n",
    "\n",
    "### Fusion with Semantic Embeddings:\n",
    "The paper’s final step is to combine these distributional emotion embeddings with semantic sentence embeddings (e.g., from MiniLM or RoBERTa applied to the sentence text itself), typically via weighted averaging or concatenation\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk ## need pip install nltk\n",
    "import re  # for regular expressions\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_data(text_file):\n",
    "    \"\"\"Convert plain text file into Whisper-like JSON structure\n",
    "    \n",
    "    Args:\n",
    "        text_file (str): Path to text file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Text data formatted like Whisper JSON output\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(text_file):\n",
    "            print(f\"Error: File not found: {text_file}\")\n",
    "            return None\n",
    "            \n",
    "        # Read the plain text\n",
    "        with open(text_file, 'r', encoding='utf-8') as file:\n",
    "            text = file.read().strip()\n",
    "            \n",
    "        # Pre-tokenize into sentences just once\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        \n",
    "        # Create Whisper-like chunks with estimated timestamps\n",
    "        current_time = 0.0\n",
    "        chunks = []\n",
    "        \n",
    "        for sent in sentences:\n",
    "            duration = len(sent.split()) * 0.3  # Rough estimate\n",
    "            chunks.append({\n",
    "                'text': sent,\n",
    "                'timestamp': [current_time, current_time + duration]\n",
    "            })\n",
    "            current_time += duration\n",
    "            \n",
    "        # Return in same format as Whisper JSON\n",
    "        return {\n",
    "            'text': text,\n",
    "            'chunks': chunks\n",
    "        }\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading text file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def segment_sentences_with_timestamps(whisper_output):\n",
    "    \"\"\"\n",
    "    Segments transcript into sentences with aligned timestamps.\n",
    "    Handles edge cases and maintains compatibility with emotion pipeline.\n",
    "    \n",
    "    Args:\n",
    "        whisper_output (dict): CrisperWhisper JSON output with 'text' and 'chunks'\n",
    "        \n",
    "    Returns:\n",
    "        list[dict]: List of {'text': str, 'timestamp': (start, end)} \n",
    "    \"\"\"\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Input validation\n",
    "    if not whisper_output or 'text' not in whisper_output or 'chunks' not in whisper_output:\n",
    "        return []\n",
    "        \n",
    "    text = whisper_output['text'].strip()\n",
    "    chunks = whisper_output['chunks']\n",
    "    \n",
    "    if not text or not chunks:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Normalize chunks for better matching\n",
    "    chunk_words = [re.sub(r'[^\\w\\s\\']', '', c['text'].lower()) for c in chunks]\n",
    "    chunk_times = [c['timestamp'] for c in chunks]\n",
    "    \n",
    "    results = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # Normalize sentence words\n",
    "        sent_words = [re.sub(r'[^\\w\\s\\']', '', w.lower()) for w in sent.split()]\n",
    "        sent_len = len(sent_words)\n",
    "        \n",
    "        match_found = False\n",
    "        \n",
    "        # Look for sentence match in remaining chunks\n",
    "        while chunk_idx <= len(chunk_words) - sent_len:\n",
    "            window = chunk_words[chunk_idx:chunk_idx + sent_len]\n",
    "            \n",
    "            # Check if window matches sentence words\n",
    "            if all(w1 == w2 for w1, w2 in zip(window, sent_words)):\n",
    "                results.append({\n",
    "                    'text': sent,\n",
    "                    'timestamp': (\n",
    "                        chunk_times[chunk_idx][0],          # Start time\n",
    "                        chunk_times[chunk_idx + sent_len - 1][1]  # End time\n",
    "                    )\n",
    "                })\n",
    "                chunk_idx += sent_len\n",
    "                match_found = True\n",
    "                break\n",
    "            chunk_idx += 1\n",
    "            \n",
    "        if not match_found:\n",
    "            # If no exact match, try fuzzy matching\n",
    "            # For now, assign last known timestamp\n",
    "            if results:\n",
    "                last_end = results[-1]['timestamp'][1]\n",
    "                results.append({\n",
    "                    'text': sent,\n",
    "                    'timestamp': (last_end, last_end + 1.0)  # Estimate 1 second duration\n",
    "                })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_corpus_file = \"../data/transcripts/penelope.txt\"\n",
    "\n",
    "# Load the target-trancript file in JSON format (TARGET data)\n",
    "transcript_file = \"../data/transcripts/audiotest_json.json\"\n",
    "with open(transcript_file, 'r') as file:\n",
    "    transcription_data = json.load(file)\n",
    "\n",
    "\n",
    "corpus_data = load_corpus_data(base_corpus_file)\n",
    "\n",
    "\n",
    "corpus_sentences = segment_sentences_with_timestamps(corpus_data)\n",
    "target_sentences = segment_sentences_with_timestamps(transcription_data)\n",
    "\n",
    "corpus_data\n",
    "#target_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step.1 Audio embeddings extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "Preprocess audio file to ensure it's in the correct format for emotion2vec (sampling required = 16kHz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "target_file = \"../data/audiotest.wav\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def preprocess_audio(audio_file, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Preprocess audio file to ensure it's in the correct format for emotion2vec\n",
    "    \n",
    "    Args:\n",
    "        audio_file (str): Path to the audio file\n",
    "        target_sr (int): Target sampling rate (emotion2vec requires 16kHz)\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the processed audio file\n",
    "    \"\"\"\n",
    "    # Load audio with its original sampling rate\n",
    "    waveform, sr = librosa.load(audio_file, sr=None)\n",
    "    \n",
    "    # Check if resampling is needed\n",
    "    if sr != target_sr:\n",
    "        # Resample to 16kHz\n",
    "        waveform = librosa.resample(waveform, orig_sr=sr, target_sr=target_sr)\n",
    "        \n",
    "        # Save the resampled audio to a temporary file\n",
    "        temp_file = \"temp_resampled.wav\"\n",
    "        sf.write(temp_file, waveform, target_sr)\n",
    "        return temp_file\n",
    "    \n",
    "    return audio_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_file_16KhZ = preprocess_audio(audio_file, target_sr=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the emotion2vec_plus_large model\n",
    "from funasr import AutoModel\n",
    "\n",
    "# model=\"iic/emotion2vec_base\"\n",
    "# model=\"iic/emotion2vec_base_finetuned\"\n",
    "# model=\"iic/emotion2vec_plus_seed\"\n",
    "# model=\"iic/emotion2vec_plus_base\" \n",
    "model_id = \"iic/emotion2vec_plus_large\"\n",
    "\n",
    "model_audio = AutoModel(\n",
    "    model=model_id,\n",
    "    hub=\"hf\",  # \"ms\" or \"modelscope\" for China mainland users; \"hf\" or \"huggingface\" for other overseas users\n",
    "    disable_update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### loading EMO2VEC\n",
    "\n",
    "[emotion2vec](https://huggingface.co/emotion2vec/emotion2vec_plus_large) is a speech emotion recognition foundation model that can extract emotional features directly from audio waveforms. The emotion2vec_plus_large model is the largest version, trained on 42,526 hours of data. It supports 9 emotion classes:\n",
    "\n",
    "0. angry\n",
    "1. disgusted\n",
    "2. fearful\n",
    "3. happy\n",
    "4. neutral\n",
    "5. other\n",
    "6. sad\n",
    "7. surprised\n",
    "8. unknown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_emotion_utterances(sentences, audio_file, model_audio, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Process audio segments into emotion sequences and embeddings, aligning with text segments.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): List of segmented sentences with timestamps\n",
    "        audio_file (str): Path to audio file\n",
    "        model_audio: Loaded emotion2vec model\n",
    "        threshold (float): Minimum probability threshold for emotion detection\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dicts containing:\n",
    "            - text: Original sentence text\n",
    "            - timestamp: (start, end) times\n",
    "            - emotion_sequence: Concatenated top emotions \n",
    "            - emotion_scores: List of (emotion, score) tuples\n",
    "            - embedding: emotion2vec embedding vector\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_audio_segment(audio_file, start_time, end_time, sr=16000):\n",
    "        \"\"\"Extract precise audio segment matching text timestamp\"\"\"\n",
    "        waveform, sr = librosa.load(audio_file, sr=sr)\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "        return waveform[start_sample:end_sample]\n",
    "    \n",
    "    audio_emotion_sequences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # 1. Get exact timestamp alignment\n",
    "        start_time, end_time = sentence['timestamp']\n",
    "        \n",
    "        # 2. Extract aligned audio segment\n",
    "        segment = extract_audio_segment(audio_file, start_time, end_time)\n",
    "        \n",
    "        # Save segment for emotion2vec processing\n",
    "        temp_path = \"temp_segment.wav\"\n",
    "        sf.write(temp_path, segment, 16000)\n",
    "        \n",
    "        # 3. Generate emotion embeddings and labels\n",
    "        result = model_audio.generate(\n",
    "            temp_path,\n",
    "            output_dir=\"./outputs\",\n",
    "            granularity=\"utterance\",\n",
    "            extract_embedding=True  # Get both embeddings and emotion scores\n",
    "        )\n",
    "        \n",
    "        if 'scores' in result[0] and 'labels' in result[0]:\n",
    "            # Process emotion labels\n",
    "            scores = result[0]['scores']\n",
    "            labels = result[0]['labels']\n",
    "            english_labels = [label.split('/')[1] if '/' in label else label \n",
    "                            for label in labels]\n",
    "            \n",
    "            # Filter emotions by threshold\n",
    "            emotion_scores = [\n",
    "                (label, score) \n",
    "                for score, label in zip(scores, english_labels)\n",
    "                if score >= threshold\n",
    "            ]\n",
    "            \n",
    "            # Sort by score and take top 3\n",
    "            emotion_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_emotions = emotion_scores[:3]\n",
    "            \n",
    "            # Create emotion sequence string (this is optional I am not doing sequencing for audio atm)\n",
    "            emotion_sequence = \"__\".join([e[0] for e in top_emotions])\n",
    "            \n",
    "            # Store results with embeddings\n",
    "            audio_emotion_sequences.append({\n",
    "                'text': sentence['text'],\n",
    "                'timestamp': sentence['timestamp'],\n",
    "                'emotion_sequence': emotion_sequence, #(this is optional I am not doing sequencing for audio atm)\n",
    "                'emotion_scores': top_emotions,\n",
    "                'embedding': result[0].get('feats', None)  # 1024-dim emotion embedding\n",
    "            })\n",
    "        \n",
    "        # Cleanup\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "            \n",
    "    return audio_emotion_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_sequences = process_audio_emotion_utterances(\n",
    "    sentences=sentences,\n",
    "    audio_file=audio_file_16KhZ,\n",
    "    model_audio=model_audio,\n",
    "    threshold=0.1 # Minimum probability threshold for emotion detection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "import numpy as np\n",
    "\n",
    "def process_emotional_flow(sentences, use_distributional=True):\n",
    "    \"\"\"Process sentences into both distributional and contextual representations\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentence dictionaries with text and timestamps\n",
    "        use_distributional: Whether to use Word2Vec (True) or transformer (False) embeddings\n",
    "    \"\"\"\n",
    "    emotion_processor = EmotionDistributionalRepresentation()\n",
    "    \n",
    "    # Step 1: Create multivariate series\n",
    "    emotion_series = emotion_processor.create_multivariate_series(sentences)\n",
    "    \n",
    "    # Step 2: Transform to emotion strings and get embeddings\n",
    "    if use_distributional:\n",
    "        emotion_sequences = emotion_processor.transform_to_emotion_strings(emotion_series, use_distributional=True)\n",
    "    else:\n",
    "        # Get emotion sequences without distributional embeddings\n",
    "        emotion_sequences = emotion_processor.transform_to_emotion_strings(emotion_series, use_distributional=False)\n",
    "        # Then apply transformer embeddings\n",
    "        emotion_sequences = generate_emotion_embeddings(emotion_sequences)\n",
    "        for seq in emotion_sequences:\n",
    "            seq['embedding_type'] = 'contextual'\n",
    "    \n",
    "    return emotion_sequences\n",
    "\n",
    "class EmotionDistributionalRepresentation:\n",
    "    def __init__(self, model_name=\"j-hartmann/emotion-english-distilroberta-base\"):\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_name,\n",
    "            return_all_scores=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.w2v_model = None\n",
    "    \n",
    "    def create_multivariate_series(self, sentences):\n",
    "        \"\"\"Treat each sentence as an observation in a multivariate series of emotions\"\"\"\n",
    "        emotion_series = []\n",
    "        for sentence in sentences:\n",
    "            scores = self.classifier(sentence['text'])[0]\n",
    "            emotion_vector = {score['label']: score['score'] for score in scores}\n",
    "            emotion_series.append({\n",
    "                'text': sentence['text'],\n",
    "                'timestamp': sentence['timestamp'],\n",
    "                'emotion_vector': emotion_vector\n",
    "            })\n",
    "        return emotion_series\n",
    "    \n",
    "    def transform_to_emotion_strings(self, emotion_series, use_distributional=True, top_k=3):\n",
    "        \"\"\"Transform emotional flow into sequence of emotion strings\"\"\"\n",
    "        emotion_sequences = []\n",
    "        all_sequences = []\n",
    "        \n",
    "        # First pass: collect all emotion sequences\n",
    "        for entry in emotion_series:\n",
    "            sorted_emotions = sorted(\n",
    "                entry['emotion_vector'].items(),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )\n",
    "            top_emotions = sorted_emotions[:top_k]\n",
    "            emotions = [emotion for emotion, _ in top_emotions]\n",
    "            all_sequences.append(emotions)\n",
    "            \n",
    "            # Create basic sequence entry\n",
    "            sequence_entry = {\n",
    "                'text': entry['text'],\n",
    "                'timestamp': entry['timestamp'],\n",
    "                'emotion_sequence': \"__\".join(emotions),\n",
    "                'emotion_vector': entry['emotion_vector'],\n",
    "                'emotions': emotions\n",
    "            }\n",
    "            emotion_sequences.append(sequence_entry)\n",
    "        \n",
    "        if use_distributional:\n",
    "            # Train Word2Vec on complete emotion sequence corpus\n",
    "            self.w2v_model = Word2Vec(\n",
    "                sentences=all_sequences,\n",
    "                vector_size=300,\n",
    "                window=2,  # Model direct emotion transitions\n",
    "                min_count=1,\n",
    "                sg=1,  # Skip-gram\n",
    "                workers=4\n",
    "            )\n",
    "            \n",
    "            # Add distributional embeddings\n",
    "            for seq in emotion_sequences:\n",
    "                seq['embedding'] = np.mean(\n",
    "                    [self.w2v_model.wv[e] for e in seq['emotions']], \n",
    "                    axis=0\n",
    "                )\n",
    "                seq['embedding_type'] = 'distributional'\n",
    "        \n",
    "        return emotion_sequences\n",
    "    \n",
    "    def get_emotion_transitions(self, emotion1, emotion2):\n",
    "        \"\"\"Get transition probability between emotions\"\"\"\n",
    "        if self.w2v_model:\n",
    "            return self.w2v_model.wv.similarity(emotion1, emotion2)\n",
    "        return None\n",
    "\n",
    "def generate_emotion_embeddings(emotion_sequences):\n",
    "    \"\"\"Generate contextual embeddings using transformer\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    for seq in emotion_sequences:\n",
    "        inputs = tokenizer(seq['emotion_sequence'], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        seq['embedding'] = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    \n",
    "    return emotion_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sentences into emotion sequences\n",
    "text_emo_sequences = process_emotional_flow(sentences)\n",
    "# Process audio segments into emotion sequences\n",
    "audio_emo_sequences = process_audio_emotion_utterances(\n",
    "    sentences=sentences,\n",
    "    audio_file=audio_file_16KhZ,\n",
    "    model_audio=model_audio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for text_seq, audio_seq in zip(text_emo_sequences, audio_emo_sequences):\n",
    "    print(f\"Sentence: {text_seq['text'][:50]}...\")\n",
    "    print(f\"Text emotions: {text_seq['emotion_sequence']}\")\n",
    "    print(f\"Audio emotions: {audio_seq['emotion_sequence']}\\n\")\n",
    "\n",
    "#audio_emo_sequences\n",
    "\n",
    "# Continue with embedding generation as before\n",
    "#emotion_embeddings = generate_emotion_embeddings(emotion_sequences)\n",
    "#emotion_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "#https://www.sciencedirect.com/science/article/pii/S0925231225004941\n",
    "\n",
    "# the authors use two emotion per sentence, we are using three\n",
    "image_url = \"https://ars.els-cdn.com/content/image/1-s2.0-S0925231225004941-gr2.jpg\"\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "These labels are about the emotion state of each sentence. *\"These emotion states are serial and interdependent and can be treated as novel tokens of a textual sequence consisting only of emotional categorizations - as if the elements in question are, after all, strings\"*\n",
    "\n",
    "[The paper](https://www.sciencedirect.com/science/article/pii/S0925231225004941) specifically mentions that their approach \"treats each sentence as an observation in a multivariate series of emotions\" and transforms \"the emotional flow of a text into a sequence of emotion strings.\" \n",
    "\n",
    "The key innovation proposed is treating emotions as having a \"distributional layout\" in text: the emotion sequences appear and alternate with each other in structured patterns of interrelations. Therefore, after we created emotion sequences that capture the dominant emotions for each sentence in order of probability and we turned them into text we use a transformer model to generate embeddings that capture the contextual relationships between emotions.\n",
    "\n",
    "This method allows to capture both the contextual and sequential dependencies between emotional states as they unfold in the text. *\"Unlike conventional sentiment classification models, which treat emotions as discrete outputs, the methodology we propose encodes sentiments as structured entities within an embedding space.\"*\n",
    "\n",
    "#### Embedding Extraction or Generation\n",
    "After constructing an emotion-state-based corpus (where each sentence has been assigned emotion labels through a multi-label classifier), the process for extracting embeddings involves two alternative approaches:\n",
    "\n",
    "1. **Extraction-based approach**: This uses pre-trained models to extract semantic representations of emotion strings. The paper explains: \"To simply extract the embeddings means that one incorporates a pre-trained scheme to extract the more or less semantic representations of the given emotion strings\". This leverages existing contextual understanding from transformer-based models. *OR*\n",
    "2. **Generation-based approach**: This involves training an embedding scheme from scratch on the serially ordered emotion states. As the paper notes: \"to generate means to train an embedding scheme over the serially ordered emotion states\". This approach captures the unique distributional patterns specific to emotion sequences.\n",
    "\n",
    "\n",
    "Here with `generate_emotion_embeddings()` we use the **extraction-based** approach, where *\"one incorporates a pre-trained scheme* --in this case \"distilber-base-ucased\"--  *to extract the more or less semantic representations of the given emotion strings.\"* \n",
    "\n",
    "1. We uses DistilBERT to capture contextual relationships between emotions in the sequence. The task of the transformer is to extract contextual representations of the emotion sequences (e.g. \"joy__sadness__anger\",\"joy__fear__disgust\"). The transformer self-attention mechanism creates contextual representations where each emotion's embedding is influenced by the other emotions in the sequence. Each self-attention layer of the transformer considers the relationships between the word \"joy\" in the emotion sequence together with the surrounding words influencing how \"joy\" is represented in the vector space. This means the embedding for \"joy\" in \"joy__sadness__anger\" would be different than in \"joy__fear__disgust\" because the surrounding emotions provide different contexts.\n",
    "\n",
    "2. We then create the CLS (**CL**a**S**sification) token embedding as a representation for the entire emotion sequence: we leverage the  pre-trained transformer's ability to aggregate information from the entire emotion sequence into a single vector. As we discussed in the previous paragraph, the model processes the emotion sequence through its attention mechanisms, where \"each attention head can focus on different aspects of meaning\" and \"multiple transformer layers progressively refine the representation\".\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Distributional Representations\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def generate_emotion_embeddings(emotion_sequences):\n",
    "    # Initialize transformer model for contextual representations\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for seq in emotion_sequences:\n",
    "        # Tokenize emotion sequence\n",
    "        inputs = tokenizer(seq['emotion_sequence'], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Use CLS token embedding as sequence representation\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        \n",
    "        seq['embedding'] = embedding\n",
    "        embeddings.append(seq)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def generate_emotion_embeddings_MM(emotion_sequences, audio_sequences):\n",
    "    \"\"\"Generate embeddings for text and audio emotion sequences combined\"\"\"\n",
    "    # Initialize transformer model for contextual representations\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for text_seq, audio_seq in zip(emotion_sequences, audio_sequences):\n",
    "        # Combine text and audio emotion sequences\n",
    "        combined_sequence = text_seq['emotion_sequence'] + \"__\" + audio_seq['emotion_sequence']\n",
    "        \n",
    "        # Tokenize combined emotion sequence \n",
    "        inputs = tokenizer(combined_sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Use CLS token embedding as sequence representation\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        \n",
    "        # Create combined sequence entry\n",
    "        combined_entry = {\n",
    "            'text': text_seq['text'],\n",
    "            'timestamp': text_seq['timestamp'],\n",
    "            'text_emotion_sequence': text_seq['emotion_sequence'],\n",
    "            'audio_emotion_sequence': audio_seq['emotion_sequence'], \n",
    "            'combined_sequence': combined_sequence,\n",
    "            'embedding': embedding,\n",
    "            'text_vector': text_seq.get('emotion_vector'),\n",
    "            'audio_scores': audio_seq.get('emotion_scores')\n",
    "        }\n",
    "        \n",
    "        embeddings.append(combined_entry)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_embeddings = generate_emotion_embeddings(text_emo_sequences)\n",
    "\n",
    "emotion_embeddings_MM = generate_emotion_embeddings_MM(text_emo_sequences, audio_emo_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "This implementation:\n",
    "\n",
    "Maintains the paper's concept of \"distributional layout\" by:\n",
    "\n",
    "Preserving sequential relationships in both modalities\n",
    "Treating emotion sequences as interdependent series\n",
    "Using contextual embeddings that capture inter-emotion relationships\n",
    "Creates a multimodal representation that:\n",
    "\n",
    "Keeps both text and audio emotion sequences aligned\n",
    "Preserves the temporal relationship between modalities\n",
    "Enables analysis of how emotions manifest differently in text vs audio\n",
    "Uses the CLS token embedding as recommended to aggregate sequence information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_emotion_analysis(emotion_embeddings_MM):\n",
    "    \"\"\"Display a detailed summary of multimodal emotion analysis\"\"\"\n",
    "    for entry in emotion_embeddings_MM:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Sentence: {entry['text'][:100]}...\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Display timestamps\n",
    "        start, end = entry['timestamp']\n",
    "        print(f\"Time window: {start:.2f}s - {end:.2f}s (duration: {end-start:.2f}s)\")\n",
    "        \n",
    "        # Display emotion sequences\n",
    "        print(\"\\nText Emotions:\", entry['text_emotion_sequence'])\n",
    "        print(\"Audio Emotions:\", entry['audio_emotion_sequence'])\n",
    "        print(\"Combined:\", entry['combined_sequence'])\n",
    "        \n",
    "        # Display top text emotions with scores\n",
    "        print(\"\\nText Emotion Scores:\")\n",
    "        for emotion, score in sorted(entry['text_vector'].items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "            print(f\"  {emotion:<10}: {score:.3f}\")\n",
    "        \n",
    "        # Display top audio emotions with scores\n",
    "        print(\"\\nAudio Emotion Scores:\")\n",
    "        for emotion, score in entry['audio_scores']:\n",
    "            print(f\"  {emotion:<10}: {score:.3f}\")\n",
    "        \n",
    "        # Show embedding dimensionality\n",
    "        print(f\"\\nEmbedding shape: {entry['embedding'].shape}\")\n",
    "\n",
    "# Display the analysis\n",
    "display_emotion_analysis(emotion_embeddings_MM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Temporal Emotion Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_emotion_flow(emotion_embeddings):\n",
    "    # Extract timestamps and emotions\n",
    "    timestamps = [e['timestamp'] for e in emotion_embeddings]\n",
    "    emotion_labels = [e['emotion_scores'][0]['label'] for e in emotion_embeddings]\n",
    "    \n",
    "    # Create time points (midpoint of each sentence)\n",
    "    time_points = [(t[0] + t[1])/2 for t in timestamps]\n",
    "    \n",
    "    # Create a mapping of emotions to colors\n",
    "    unique_emotions = list(set(emotion_labels))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_emotions)))\n",
    "    emotion_colors = {emotion: colors[i] for i, emotion in enumerate(unique_emotions)}\n",
    "    \n",
    "    # Plot emotions over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, (time, emotion) in enumerate(zip(time_points, emotion_labels)):\n",
    "        plt.scatter(time, 1, color=emotion_colors[emotion], s=100)\n",
    "        plt.text(time, 1.05, emotion, rotation=45, ha='center')\n",
    "    \n",
    "    # Add sentence text as annotations\n",
    "    for i, embedding in enumerate(emotion_embeddings):\n",
    "        plt.annotate(embedding['text'][:30] + '...', \n",
    "                     xy=(time_points[i], 0.9),\n",
    "                     xytext=(time_points[i], 0.7),\n",
    "                     arrowprops=dict(arrowstyle='->'),\n",
    "                     ha='center')\n",
    "    \n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.title('Emotion Flow Over Time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_emotion_flow(emotion_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Alternative using multimodal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Audio: Sharing Embeddings Instead of Raw Recordings\n",
    "Privacy-Preserving Embeddings:\n",
    "Modern research shows that audio embeddings can be designed to retain emotion-relevant features while suppressing speaker identity and other biometric information. Techniques include adversarial training and feature importance-based modification of embeddings, which allow for emotion recognition tasks without exposing speaker identity or reconstructing the original audio signal.\n",
    "\n",
    "Utility and Compliance:\n",
    "Such embeddings are considered privacy-enabled and utility-preserving, supporting emotion detection while reducing risks associated with voiceprints and biometric data. This approach is also compatible with GDPR and similar regulations, as it avoids sharing personally identifiable information.\n",
    "\n",
    "# Text: Anonymization Strategies\n",
    "Automated Anonymization Tools:\n",
    "Use transformer-based models (e.g., BERT, GPT-2) or specialized tools like Microsoft Presidio to detect and redact personal information (names, locations, dates, etc.) from transcripts. These tools can replace sensitive data with placeholders, ensuring the text remains useful for analysis but cannot be traced back to individuals.\n",
    "\n",
    "Manual Review and Metadata Scrubbing:\n",
    "After automated anonymization, manually review transcripts for indirect identifiers (e.g., rare job titles, unique life events) and remove or generalize them as needed. Also, strip or generalize metadata (e.g., device, location, demographics) to further reduce re-identification risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
