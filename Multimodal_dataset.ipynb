{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Recognition Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "**Aim** to create a multimodal emotion recognition dataset from natural speech collected in naturalistic enviroment. The dataset will integrate two modalities: voice and corresponding transcripts\n",
    "\n",
    "## Introduction\n",
    "The audio and text-based emotion encoder proposed follows and expands the work by Liampis, Karanikola, and Kotsiantis ([2025](https://www.sciencedirect.com/science/article/pii/S0925231225004941)) who model the serial, distributional structure of emotions in text by converting sequences of sentences into sequences of emotion labels, and then generating distributional emotion embeddings from these sequences. The idea behind their work is that emotions in text are not isolated states but unfold as structured, serially-dependent patterns akin to a time series and forming a sequence that reflects the emotional evolution throughout the discourse. The authors argue that the emotional states exhibit distributional patterns as they appear and alternate in text, very much like semantic relationships between words are shaped by their order and co-occurrence. In our multimodal pipeline, we extend this idea with an audio modality: we extract high-level emotion embeddings from each segment using emotion2vec which capture contextual acoustic emotion features. We then use timestamps to align and concatenate these representations at the segment level to form a multimodal feature space.  \n",
    "\n",
    "\n",
    "### Conceptual Framework\n",
    "\n",
    "#### Text component\n",
    "* *Serial Emotional Footprint in Text*: Building on Liapis, Karanikola, and Kotsiantis (2025), we treat the emotional flow in natural language as a structured, serially-dependent pattern. Each segment of text (e.g., sentence or utterance) is assigned a set of emotion labels using a multi-label classifier. These labels are then arranged in order, forming an \"emotion string\" that reflects the temporal progression of emotions throughout the transcript. This sequence is modeled as a time-series-like structure, capturing the distributional regularities and interdependencies of emotions as they appear and alternate in the text\n",
    "* *Distributional Emotion Embeddings for Text*: The emotion label sequences (emotion strings) derived from the text are embedded using distributional models (e.g., Word2Vec trained on the emotion label corpus) and/or contextual transformer-based models. This process encodes both the semantic and sequential properties of emotional expression in the text, allowing downstream models to leverage latent information about emotion transitions and dependencies that cannot be captured by isolated emotion labels alone\n",
    "#### Audio Component\n",
    "* *Contextual Acoustic Emotion Embeddings for Audio*: For the audio modality, we extract high-level, segment-level emotion embeddings using a state-of-the-art model (e.g. emotion2vec). These embeddings capture the contextual acoustic features relevant to emotional expression in speech but are not modeled as distributional or sequential emotion label strings. Instead, each audio segment (the audio stream corresponding to a phrasal unit) is represented by its corresponding emotion2vec embedding, which encodes paralinguistic cues and affective information from the speech signal.\n",
    "\n",
    "#### Multimodal integration\n",
    "* *Multimodal Alignment and Fusion*: Audio and text segments are aligned using precise timestamps obtained from the transcription process (via crisper-whisper). For each aligned segment, the contextual acoustic embedding (from audio) and the distributional emotion embedding (from text) are concatenated to form a multimodal feature vector. This approach should integrate the complementary strengths of both modalities: the structured, sequential modeling of emotions in text and the rich paralinguistic information in speech. By aligning and fusing these representations at the segment level, we create a comprehensive multimodal feature space for emotion recognition, suitable for robust analysis in naturalistic, diary-style speech data.\n",
    "\n",
    "## Dataset Structure\n",
    "Two data modes:\n",
    "* **Audio**: segment-level emotion embeddings and labels.\n",
    "* **Text**: Tokenized transcripts, segment-level emotion embeddings and labels.\n",
    "\n",
    "### Data Collection\n",
    "\n",
    "* **Speech Recordings**: High-quality (16KhZ), natural (and naturalistic) speech samples. \n",
    "* **Text Transcripts**: Time-aligned transcripts for each audio recording (synchronization at the utterance level.\n",
    "* **Emotion Annotation**: The present pipeline is using only automatic emotion classifiers (e.g., fine-tuned transformers, emotion2vec). In addition we will need to add human annotators to label each segment with one or more emotion categories.\n",
    "\n",
    "\n",
    "## Method\n",
    "\n",
    "### Tools\n",
    "\n",
    "* [emotion2vec: audio-based emotion embeddings](https://huggingface.co/emotion2vec/emotion2vec_plus_large)\n",
    "* [emotion-english-distilroberta-base](https://huggingface.co/j-hartmann/emotion-english-roberta-large)\n",
    "* [CrisperWhisper](https://github.com/nyrahealth/CrisperWhisper)\n",
    "\n",
    "**Emotion2vec** is the first universal speech emotion representation model. Through self-supervised pre-training, emotion2vec has the ability to extract emotion representation across different tasks, languages, and scenarios. Emotion2vec aims to create a universal emotional representation space. \n",
    "**Emotion-english-distilroberta-base** model is a distilled version of RoBERTa-large trained on 6 diverse datasets and predicts Ekman's 6 basic emotions, plus a neutral class. RoBERTa-large (Liu et al. 2019) is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. \n",
    "**Crisper-whisper** is an advanced variant of OpenAI's Whisper, designed for fast, precise, and verbatim speech recognition with accurate (crisp) word-level timestamps. Unlike the original Whisper, which tends to omit disfluencies and follows more of a intended transcription style, CrisperWhisper aims to transcribe every spoken word exactly as it is, including fillers, pauses, stutters and false starts.\n",
    "**Text Anonymization Pipeline**: For automated and manual redaction of personal identifiers in transcripts with Presidio (To be done). Presidio is an open-source NLP library developed by Microsoft, designed for automated text redaction and anonymization\n",
    "\n",
    "### Data\n",
    "\n",
    "| Item             | Description                                                             | Included in Dataset  | Requires Anonymization |\n",
    "|------------------|-------------------------------------------------------------------------|----------------------|------------------------|\n",
    "| Audio File       | A .wav file containing speech (e.g., audiotest.wav)                     | No                   | No                     |\n",
    "| Audio embeddings | Extracted, non-reconstructable, privacy-checked embeddings              | Yes                  | No                     |\n",
    "| Transcripts      | Output of crisperwhisper (JSON format) with utterance-level timestamps  | Yes                  | Yes                    |\n",
    "| Timestamps       | Used for alignment, included in metadata                                | Yes                  | No                     |\n",
    "\n",
    "\n",
    "### Multimodal Processing\n",
    "\n",
    "### MODE 1: Text analysis\n",
    "\n",
    "This is the process to generate distributional embeddings [Liampis 2025](https://www.sciencedirect.com/science/article/pii/S0925231225004941).\n",
    "\n",
    "Two different representations are needed for the final embedding to be constructed. \n",
    "\n",
    "1. **base corpus**. The first representation concerns an (ideally extensive) textual corpus that is called *base corpus*, from which the base embeddings are generated (and/or) extracted.\n",
    "2. **target data**. The second representation concerns the data one wants to model or *target data*.\n",
    "\n",
    "The process follows **three algorythms**:\n",
    "\n",
    "#### Base Corpus Processing (Algorithm 1)\n",
    "This is the computationally expensive part (building the distributional embedding space). Once this is done, new data can be mapped quickly using the pre-trained models and the emotion vocabularies that we have embedded.\n",
    "\n",
    "* Purpose: Build the emotion embedding space (Word2Vec and/or transformer-based) from a large, diverse corpus.\n",
    "* How Often: Only once or whenever we want to update the base_corpus distributional embedding space.\n",
    "* Pipeline:\n",
    "    * Tokenizes the base corpus into sentences\n",
    "    * Classifies each sentence for emotions (using a multi-label classifier)\n",
    "    * Creates serial emotion strings (e.g., Joy__Trust__Surprise)\n",
    "    * Trains or extracts embeddings for these emotion strings (Word2Vec and/or transformer)\n",
    "**ouput**: a mapping from emotion strings to embeddings (this is our \"base embedding space\").\n",
    "\n",
    "#### Target Data Processing (Algorithm 2)\n",
    "* Purpose: Map new (target) data into the embedding space created from the base corpus.\n",
    "* How Often: Every time we have new target data (step 1 of 2 for new data).\n",
    "* Output:\n",
    "    * Tokenizes the new transcript into sentences\n",
    "    * Classifies each sentence for emotions\n",
    "    * Creates emotion strings for each sentence\n",
    "    * Maps these emotion strings to the pre-trained embeddings from the base corpus (using the same vocabulary and embedding models as in algo. 1)\n",
    "    * Combines (weighted average) the different embedding layers (e.g., Word2Vec and transformer)\n",
    "**output**: distributional emotion embeddings for each sentence of the target transcript.\n",
    "\n",
    "#### Final Embedding Construction (Algorithm 3)\n",
    "* Purpose: For downstream tasks, concatenate the distributional emotion embeddings with sentence-level semantic embeddings.\n",
    "* How Often: Every time we process new target data (step 2 of 2 for new data).\n",
    "* What it does:\n",
    "    * Gets the distributional emotion embeddings (from Algorithm 2)\n",
    "    * Extracts sentence embeddings for the target data (using a transformer)\n",
    "    * Flattens and concatenates these embeddings for each sentence\n",
    "**output**: A final feature space for each sentence, ready for use in downstream models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from funasr import AutoModel\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## MODE 1: Txt-based distributional embeddings\n",
    "\n",
    "### Transcript generation\n",
    "\n",
    "* Transcribe the Audio: Convert the audio file into text using Crisperwhisper to obtain transcripts with word-level timestamps.\n",
    "    * Anonymisation: we use Microsoft Presidio (automated tool) - **This needs testing, not implemented yet**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk ## need pip install nltk\n",
    "import re  # for regular expressions\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def load_and_flatten_transcripts(filename):\n",
    "    ## we need to merge lists into one (because of how they were transcribed with whisper (in mnually separated chunks)\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    # Find all lists of dicts using regex\n",
    "    lists = re.findall(r'\\[.*?\\]', content, re.DOTALL)\n",
    "    all_entries = []\n",
    "    for l in lists:\n",
    "        try:\n",
    "            entries = ast.literal_eval(l)\n",
    "            all_entries.extend(entries)\n",
    "        except Exception as e:\n",
    "            print(\"Error in segment:\", e)\n",
    "    return all_entries\n",
    "\n",
    "\n",
    "def retime_transcript(transcript, threshold=1.0):\n",
    "    retimed = []\n",
    "    time_offset = 0.0\n",
    "    prev_end = 0.0\n",
    "\n",
    "    for entry in transcript:\n",
    "        start, end = entry['timestamp']\n",
    "        # If timestamps reset, increment offset\n",
    "        if start < prev_end - threshold:\n",
    "            time_offset += prev_end\n",
    "        adj_start = start + time_offset\n",
    "        adj_end = end + time_offset\n",
    "        retimed.append({'text': entry['text'], 'timestamp': (adj_start, adj_end)})\n",
    "        prev_end = end\n",
    "    return retimed\n",
    "\n",
    "\n",
    "def segment_sentences_with_timestamps(whisper_output):\n",
    "    \"\"\"\n",
    "    Segments transcript into sentences with aligned timestamps.\n",
    "    Handles edge cases and maintains compatibility with emotion pipeline.\n",
    "    \n",
    "    Args:\n",
    "        whisper_output (dict): CrisperWhisper JSON output with 'text' and 'chunks'\n",
    "        \n",
    "    Returns:\n",
    "        list[dict]: List of {'text': str, 'timestamp': (start, end)} \n",
    "    \"\"\"\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Input validation\n",
    "    if not whisper_output or 'text' not in whisper_output or 'chunks' not in whisper_output:\n",
    "        return []\n",
    "        \n",
    "    text = whisper_output['text'].strip()\n",
    "    chunks = whisper_output['chunks']\n",
    "    \n",
    "    if not text or not chunks:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Normalize chunks for better matching\n",
    "    chunk_words = [re.sub(r'[^\\w\\s\\']', '', c['text'].lower()) for c in chunks]\n",
    "    chunk_times = [c['timestamp'] for c in chunks]\n",
    "    \n",
    "    results = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # Normalize sentence words\n",
    "        sent_words = [re.sub(r'[^\\w\\s\\']', '', w.lower()) for w in sent.split()]\n",
    "        sent_len = len(sent_words)\n",
    "        \n",
    "        match_found = False\n",
    "        \n",
    "        # Look for sentence match in remaining chunks\n",
    "        while chunk_idx <= len(chunk_words) - sent_len:\n",
    "            window = chunk_words[chunk_idx:chunk_idx + sent_len]\n",
    "            \n",
    "            # Check if window matches sentence words\n",
    "            if all(w1 == w2 for w1, w2 in zip(window, sent_words)):\n",
    "                results.append({\n",
    "                    'text': sent,\n",
    "                    'timestamp': (\n",
    "                        chunk_times[chunk_idx][0],          # Start time\n",
    "                        chunk_times[chunk_idx + sent_len - 1][1]  # End time\n",
    "                    )\n",
    "                })\n",
    "                chunk_idx += sent_len\n",
    "                match_found = True\n",
    "                break\n",
    "            chunk_idx += 1\n",
    "            \n",
    "        if not match_found:\n",
    "            # If no exact match, try fuzzy matching\n",
    "            # For now, assign last known timestamp\n",
    "            if results:\n",
    "                last_end = results[-1]['timestamp'][1]\n",
    "                results.append({\n",
    "                    'text': sent,\n",
    "                    'timestamp': (last_end, last_end + 1.0)  # Estimate 1 second duration\n",
    "                })\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Sentence Tokenisation\n",
    "The base corpus and the target data should be tokenized into sentences (via `segment_into_sentences()`). \n",
    "\n",
    "> The output of whisper transcribed by jiawei is already tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the target-trancript file in JSON format (TARGET data)\n",
    "target_file = \"../data/transcripts/audiotest_json.json\"\n",
    "with open(target_file, 'r') as file:\n",
    "    transcription_data = json.load(file)\n",
    "\n",
    "target_data = segment_sentences_with_timestamps(transcription_data)\n",
    "#target_data = transcription_data\n",
    "\n",
    "\n",
    "base_corpus_unparsed = load_and_flatten_transcripts(\"../data/transcripts/base_corpus.txt\") # this needs timestamps to be re-parsed \n",
    "base_corpus = retime_transcript(base_corpus_unparsed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#base_corpus\n",
    "#transcription_data\n",
    "#target_data\n",
    "\n",
    "len(base_corpus)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Base Corpus Processing (Algorithm 1)\n",
    "\n",
    "![algo1](https://ars.els-cdn.com/content/image/1-s2.0-S0925231225004941-fx1001.jpg)\n",
    "\n",
    "> in the code this is done via **algorithm_1_embeddings_extraction()**\n",
    "\n",
    "Followin this algorithm we start by \n",
    "1. segment tokenisation (we have done this): segment the transcript into sentences - via nltk (a natural language tokenizer).\n",
    "    * allocate timestamps to each sentence (precise start and stop timing of each segment), this will preserve alignment with between audio and txt segments for later on. \n",
    "2. we extract, through the use of transformer-based multi-label emotion classification, the corresponding emotions for each sentence. **Ideally we will label the base corpus ourselves to form a new labelled dataset**.  In this case use a pre-trained transformer to assuming that the output of the emotion classifier is the groud-truth. For each segment:\n",
    "    1.  we apply the [Emotion English DistilRoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large)  model to obtain Hartman's  emotion probabilities (or any other suitable multi-label emotion classifier).\n",
    "    2. we serialise the top-$k$ (k=3) emotion labels of each segment into a new txt string Emotion1__Emotion2__Emotion3 (e.g.\"joy__surprise__fear\") in order of their probability value. We call this token **emotion string** and the vocolabulary of this token is as \"large\" as $k$ - larger k will involve more subtle representation of the underlying true emotion.\n",
    "3. We concate the emotion strings in the temporal order of appearance to create a time-series-like structure (**base emotion corpus**). This corpus organises emotion strings as tokens in a \"text\" where the vocabulary consists only of emotion labels. e.g.:\n",
    "\n",
    "![serialised_labels](https://ars.els-cdn.com/content/image/1-s2.0-S0925231225004941-gr2.jpg)\n",
    "\n",
    "Thus, while each *emotion string* captures the emotion dominance (top-k probability value) of each segment. In addition, the concatenation of these emotion strings (*base emotion corpus*) captures the temporal sequence of those strings and can be subject to distributional logic. \n",
    "\n",
    "4. Base embedding extraction and/or generation:\n",
    "    - Extraction : Use a pretrained sentence transformer (e.g., all-MiniLM-L6-v2) to embed emotion strings, capturing semantic relationships between them: the embeddings will be a representation of the emotion combinations and _do not_ include any serial (temporal) interdependencies between the emotion label tokens (i.e. emotion strings).\n",
    "    - Generation : Train a Word2Vec model on the emotion string sequences to encode sequential dependencies (e.g., how \"Joy\" often precedes \"Surprise\").\n",
    "\n",
    "\n",
    "\n",
    "### Distributional Embeddings (Algorithm 2)\n",
    "\n",
    "![algo2](https://ars.els-cdn.com/content/image/1-s2.0-S0925231225004941-fx1002.jpg)\n",
    "\n",
    "> in the code this is doene via **algorithm_2_distributional_embeddings()**\n",
    "\n",
    "Similarly to what we did for the base corpus:\n",
    "1. we compute the base embeddings\n",
    "    1. tokenize the target data\n",
    "    2. we use the same RoBERTa model we used for the base corpus to predict the corresponding emotion probabilities for the target textual data.\n",
    "    3.  Using these predictions, we again extract the *emotion string* for each text sample\n",
    "2. We map the embeddings generated from the base corpus onto the new emotion labels extracted from the target dataset.\n",
    "    1. This is done by matching the emotion labels between the base and target datasets and retrieving the corresponding embeddings from the relevant dictionaries, thus using the preexisting embeddings derived from the base corpus.\n",
    "3. Once the embeddings are mapped to the target dataset, the resulting embeddings are flattened, expanding the embedding vectors into separate columns while maintaining the corresponding dimensionality. This transformation allows the embeddings to be treated as distinct features.\n",
    "4. Then we extend this embedding representation by leveraging a Word2Vec model to capture the sequential interdependencies of the emotion strings. Using the emotion label corpus extracted from these vocabularies, we train a Word2Vec scheme with a vector size of 384 dimensions. The Word2Vec scheme is trained with a window size of 10, allowing it to capture contextual relationships between different emotion combinations within the emotion-based corpus.\n",
    "   \n",
    "### Construct Final Embedding space (Algorithm 3)\n",
    "The final embeddings are a weighted average version of both MiniLM and Word2Vec. Thus, to further enhance the emotion embeddings so that they contain both semantic and distributional information, we employ a weighted averaging scheme that blends the embeddings generated by the MiniLM-L6-v2 model with those from the Word2Vec model.\n",
    "\n",
    "![algo2_1](https://ars.els-cdn.com/content/image/1-s2.0-S0925231225004941-gr5.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class DistributionalEmotionEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.emotion_classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "            return_all_scores=True\n",
    "        )\n",
    "        self.base_embeddings = {}  # Store base corpus embeddings\n",
    "        self.base_emotion_vocab = {}  # Store base emotion vocabulary\n",
    "        self.word2vec_model = None\n",
    "        self.sentence_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    def algorithm_1_embeddings_extraction(self, corpus, process_type=\"both\"):\n",
    "        \"\"\"Algorithm 1: Embeddings Extraction from base corpus\"\"\"\n",
    "        sentences = corpus\n",
    "        \n",
    "        # Step 2: Emotion Classification\n",
    "        emotion_labels_corpus = []\n",
    "        for sentence in sentences:\n",
    "            scores = self.emotion_classifier(sentence['text'])[0]\n",
    "            sorted_emotions = sorted(scores, key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            single = sorted_emotions[0]['label']\n",
    "            double = \"__\".join([e['label'] for e in sorted_emotions[:2]])\n",
    "            triple = \"__\".join([e['label'] for e in sorted_emotions[:3]])\n",
    "            \n",
    "            emotion_labels_corpus.append({\n",
    "                'single': single,\n",
    "                'double': double,\n",
    "                'triple': triple\n",
    "            })\n",
    "        \n",
    "        # Step 3: Create Emotion Label Texts\n",
    "        emotion_sequences = {\n",
    "            'single': [item['single'] for item in emotion_labels_corpus],\n",
    "            'double': [item['double'] for item in emotion_labels_corpus],\n",
    "            'triple': [item['triple'] for item in emotion_labels_corpus]\n",
    "        }\n",
    "        \n",
    "        # Store vocabulary for mapping\n",
    "        self.base_emotion_vocab = emotion_sequences\n",
    "        \n",
    "        # Step 4: Embedding Extraction or Generation\n",
    "        embeddings = {}\n",
    "        \n",
    "        if process_type in [\"extraction\", \"both\"]:\n",
    "            # These embeddings, however, are extracted, not generated \n",
    "            # or trained from scratch in a way that captures the sequential \n",
    "            # or temporal dynamics of emotion fluctuations across the text\n",
    "            # That is, the extracted embeddings in this step are merely contextual \n",
    "            # semantic embeddings that do not include any serial interdependencies \n",
    "            # between the emotion label tokens.\n",
    "            for vocab_type in ['single', 'double', 'triple']:\n",
    "                embeddings[f'{vocab_type}_extraction'] = self.sentence_transformer.encode(\n",
    "                    emotion_sequences[vocab_type]\n",
    "                )\n",
    "        \n",
    "        if process_type in [\"generation\", \"both\"]:\n",
    "            # After constructing the emotion embeddings from the base corpus, \n",
    "            # we proceed to further enrich the embedding representation by leveraging \n",
    "            # a Word2Vec model so that we can model sequential interdependencies too.\n",
    "            tokenized_sequences = []\n",
    "            for vocab_type in ['single', 'double', 'triple']:\n",
    "                tokenized_sequences.extend([seq.split(\"__\") for seq in emotion_sequences[vocab_type]])\n",
    "            \n",
    "            self.word2vec_model = Word2Vec(\n",
    "                # this configuration replicates the spec of the paper\n",
    "                sentences=tokenized_sequences,\n",
    "                vector_size=384,\n",
    "                # The Word2Vec scheme is trained with a window size of 10, allowing it to capture \n",
    "                # contextual relationships between different emotion combinations within the \n",
    "                # emotion-based corpus\n",
    "                window=10,\n",
    "                min_count=1,\n",
    "                workers=4\n",
    "            )\n",
    "            \n",
    "            for vocab_type in ['single', 'double', 'triple']:\n",
    "                # To model sequential interdependencies we utilize the emotion word combinations \n",
    "                # (single, double, and triple emotions) extracted from the base corpus\n",
    "                w2v_embeddings = []\n",
    "                for seq in emotion_sequences[vocab_type]:\n",
    "                    tokens = seq.split(\"__\")\n",
    "                    token_embeddings = [self.word2vec_model.wv[token] for token in tokens if token in self.word2vec_model.wv]\n",
    "                    if token_embeddings:\n",
    "                        w2v_embeddings.append(np.mean(token_embeddings, axis=0))\n",
    "                    else:\n",
    "                        w2v_embeddings.append(np.zeros(384))\n",
    "                embeddings[f'{vocab_type}_generation'] = np.array(w2v_embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def algorithm_2_distributional_embeddings(self, base_corpus, target_data, num_layers=2):\n",
    "        \"\"\"Algorithm 2: Distributional Emotion Embeddings\"\"\"\n",
    "        # Step 1: Compute Base Embeddings\n",
    "        self.base_embeddings = self.algorithm_1_embeddings_extraction(base_corpus, \"both\")\n",
    "        \n",
    "        # Step 2: Map Target Emotion Data to Base Embeddings\n",
    "        # Once the Word2Vec model is trained (step1), the embeddings generated are used in conjunction with \n",
    "        # the target dataset in the context of a process similar to the one applied in the \n",
    "        # earlier steps described for the base corpus:\n",
    "        target_results = []\n",
    "        for sentence in target_data:\n",
    "            scores = self.emotion_classifier(sentence['text'])[0]\n",
    "            # the emotion probabilities are extracted for each text entry of the target data set using the multi-label classifier\n",
    "            sorted_emotions = sorted(scores, key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            emotion_strings = {\n",
    "                # Then the single-, double-, and triple-emoword constituents of the emotion states are \n",
    "                # extracted for each instance\n",
    "                'single': sorted_emotions[0]['label'],\n",
    "                'double': \"__\".join([e['label'] for e in sorted_emotions[:2]]),\n",
    "                'triple': \"__\".join([e['label'] for e in sorted_emotions[:3]])\n",
    "            }\n",
    "            \n",
    "            # Map to base embeddings using Word2Vec model\n",
    "            # The corresponding labels of the emowords are then mapped to their corresponding embeddings derived from the Word2Vec model.\n",
    "            mapped_embeddings = {}\n",
    "            for vocab_type in ['single', 'double', 'triple']:\n",
    "                emotion_string = emotion_strings[vocab_type]\n",
    "                \n",
    "                # Generation embeddings (Word2Vec)\n",
    "                if self.word2vec_model:\n",
    "                    tokens = emotion_string.split(\"__\")\n",
    "                    token_embeddings = [self.word2vec_model.wv[token] for token in tokens if token in self.word2vec_model.wv]\n",
    "                    if token_embeddings:\n",
    "                        mapped_embeddings[f'{vocab_type}_generation'] = np.mean(token_embeddings, axis=0)\n",
    "                    else:\n",
    "                        mapped_embeddings[f'{vocab_type}_generation'] = np.zeros(384)\n",
    "                \n",
    "                # Extraction embeddings (Sentence Transformer)\n",
    "                mapped_embeddings[f'{vocab_type}_extraction'] = self.sentence_transformer.encode([emotion_string])[0]\n",
    "            \n",
    "            target_results.append({\n",
    "                'text': sentence['text'],\n",
    "                'timestamp': sentence['timestamp'],\n",
    "                'emotion_strings': emotion_strings,\n",
    "                'embeddings': mapped_embeddings\n",
    "            })\n",
    "        \n",
    "        # Step 3: Weighted Average of Embeddings\n",
    "        alpha_extraction = 0.4\n",
    "        alpha_generation = 0.6\n",
    "        \n",
    "        final_embeddings = []\n",
    "        for result in target_results:\n",
    "            # Combine extraction and generation embeddings (using single emotions as example)\n",
    "            extraction_emb = result['embeddings']['single_extraction']\n",
    "            generation_emb = result['embeddings']['single_generation']\n",
    "            \n",
    "            combined = alpha_extraction * extraction_emb + alpha_generation * generation_emb\n",
    "            final_embeddings.append(combined)\n",
    "        \n",
    "        return np.array(final_embeddings), target_results\n",
    "    \n",
    "    def algorithm_3_final_embedding_space(self, base_corpus, target_data, num_layers=2):\n",
    "        \"\"\"Algorithm 3: Construct Final Embedding Space\"\"\"\n",
    "        # Step 1: Compute Distributional Emotion Embeddings\n",
    "        distributional_embeddings, target_results = self.algorithm_2_distributional_embeddings(\n",
    "            base_corpus, target_data, num_layers\n",
    "        )\n",
    "        \n",
    "        # Step 2: Extract Textual Sentence Embeddings\n",
    "        target_texts = [sentence['text'] for sentence in target_data]\n",
    "        sentence_embeddings = self.sentence_transformer.encode(target_texts)\n",
    "        \n",
    "        # Step 3: Flatten Embeddings (if needed)\n",
    "        de_flat = distributional_embeddings.reshape(distributional_embeddings.shape[0], -1)\n",
    "        se_flat = sentence_embeddings.reshape(sentence_embeddings.shape[0], -1)\n",
    "        \n",
    "        # Step 4: Concatenate Embeddings\n",
    "        final_embeddings = np.concatenate([de_flat, se_flat], axis=1)\n",
    "        \n",
    "        return final_embeddings, target_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = DistributionalEmotionEmbeddings()\n",
    "\n",
    "# Algorithm 1: Extract embeddings from base corpus (this is called internally)\n",
    "# embedder.algorithm_1_embeddings_extraction(base_corpus)\n",
    "\n",
    "\n",
    "\n",
    "# Algorithm 2: Generate distributional emotion embeddings for target data\n",
    "#    adjust the num_layers so that:\n",
    "#         Single: Vocabulary size = number of emotion labels (e.g., 11).\n",
    "#         Double: Vocabulary size = number of possible ordered pairs (e.g., 11 × 10 = 110 - order matters).\n",
    "#         Triple: Vocabulary size = number of possible ordered triplets (e.g., 11 × 10 × 9 = 990).\n",
    "\n",
    "distributional_embeddings = embedder.algorithm_2_distributional_embeddings(\n",
    "    base_corpus=base_corpus, \n",
    "    target_data=target_data, \n",
    "    num_layers=2  # My base corpus sample is relatively small (1514 sentences)\n",
    ")\n",
    "\n",
    "# Algorithm 3: Generate final multimodal embeddings (combines with sentence semantics)\n",
    "final_embeddings, detailed_results = embedder.algorithm_3_final_embedding_space(\n",
    "    base_corpus=base_corpus, \n",
    "    target_data=target_data\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(detailed_results):\n",
    "    print(f\"Sentence {i}: {result['text']}\")\n",
    "    print(f\"Emotion strings: {result['emotion_strings']}\")\n",
    "    print(f\"Final embedding shape: {final_embeddings[i].shape}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Step 2. Audio embeddings extraction (mode 2)\n",
    "\n",
    "We use emotion2vec to extract high-level emotion embeddings from the audio file.\n",
    "\n",
    "Note: We only publish these embeddings, not the raw audio. We will also look into further processing of the embeddings to minimize speaker-identifiable information (using e.g. dimensionality reduction via umap).\n",
    "\n",
    "\n",
    "#### Step 3. Multimodal Integration\n",
    "\n",
    "1. Align audio and text segments using timestamps.\n",
    "    * Segment Matching: Map audio segments (from emotion2vec) to text segments using timestamps (e.g., a 3-second audio chunk and its corresponding transcribed sentence).\n",
    "    * Embedding Concatenation: For each aligned segment, concatenate:\n",
    "        * Audio: Emotion2vec embedding (contextual acoustic features).\n",
    "        * Text: Distributional emotion embedding (serial emotion patterns).\n",
    "\n",
    "As we concatenate the audio emotion embedding and the corresponding text-based emotion embedding we form a multimodal feature vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "#https://www.sciencedirect.com/science/article/pii/S0925231225004941\n",
    "\n",
    "# the authors use two emotion per sentence, we are using three\n",
    "image_url = \"https://ars.els-cdn.com/content/image/1-s2.0-S0925231225004941-gr2.jpg\"\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Step.2 Audio embeddings extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "Preprocess audio file to ensure it's in the correct format for emotion2vec (sampling required = 16kHz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "target_file = \"../data/audiotest.wav\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def preprocess_audio(audio_file, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Preprocess audio file to ensure it's in the correct format for emotion2vec\n",
    "    \n",
    "    Args:\n",
    "        audio_file (str): Path to the audio file\n",
    "        target_sr (int): Target sampling rate (emotion2vec requires 16kHz)\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the processed audio file\n",
    "    \"\"\"\n",
    "    # Load audio with its original sampling rate\n",
    "    waveform, sr = librosa.load(audio_file, sr=None)\n",
    "    \n",
    "    # Check if resampling is needed\n",
    "    if sr != target_sr:\n",
    "        # Resample to 16kHz\n",
    "        waveform = librosa.resample(waveform, orig_sr=sr, target_sr=target_sr)\n",
    "        \n",
    "        # Save the resampled audio to a temporary file\n",
    "        temp_file = \"temp_resampled.wav\"\n",
    "        sf.write(temp_file, waveform, target_sr)\n",
    "        return temp_file\n",
    "    \n",
    "    return audio_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_file_16KhZ = preprocess_audio(target_file, target_sr=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the emotion2vec_plus_large model\n",
    "from funasr import AutoModel\n",
    "\n",
    "# model=\"iic/emotion2vec_base\"\n",
    "# model=\"iic/emotion2vec_base_finetuned\"\n",
    "# model=\"iic/emotion2vec_plus_seed\"\n",
    "# model=\"iic/emotion2vec_plus_base\" \n",
    "model_id = \"iic/emotion2vec_plus_large\"\n",
    "\n",
    "model_audio = AutoModel(\n",
    "    model=model_id,\n",
    "    hub=\"hf\",  # \"ms\" or \"modelscope\" for China mainland users; \"hf\" or \"huggingface\" for other overseas users\n",
    "    disable_update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### loading EMO2VEC\n",
    "\n",
    "[emotion2vec](https://huggingface.co/emotion2vec/emotion2vec_plus_large) is a speech emotion recognition foundation model that can extract emotional features directly from audio waveforms. The emotion2vec_plus_large model is the largest version, trained on 42,526 hours of data. It supports 9 emotion classes:\n",
    "\n",
    "0. angry\n",
    "1. disgusted\n",
    "2. fearful\n",
    "3. happy\n",
    "4. neutral\n",
    "5. other\n",
    "6. sad\n",
    "7. surprised\n",
    "8. unknown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_emotion_utterances(sentences, audio_file, model_audio, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Process audio segments into emotion sequences and embeddings, aligning with text segments.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): List of segmented sentences with timestamps\n",
    "        audio_file (str): Path to audio file\n",
    "        model_audio: Loaded emotion2vec model\n",
    "        threshold (float): Minimum probability threshold for emotion detection\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dicts containing:\n",
    "            - text: Original sentence text\n",
    "            - timestamp: (start, end) times\n",
    "            - emotion_sequence: Concatenated top emotions \n",
    "            - emotion_scores: List of (emotion, score) tuples\n",
    "            - embedding: emotion2vec embedding vector\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_audio_segment(audio_file, start_time, end_time, sr=16000):\n",
    "        \"\"\"Extract precise audio segment matching text timestamp\"\"\"\n",
    "        waveform, sr = librosa.load(audio_file, sr=sr)\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "        return waveform[start_sample:end_sample]\n",
    "    \n",
    "    audio_emotion_sequences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # 1. Get exact timestamp alignment\n",
    "        start_time, end_time = sentence['timestamp']\n",
    "        \n",
    "        # 2. Extract aligned audio segment\n",
    "        segment = extract_audio_segment(audio_file, start_time, end_time)\n",
    "        \n",
    "        # Save segment for emotion2vec processing\n",
    "        temp_path = \"temp_segment.wav\"\n",
    "        sf.write(temp_path, segment, 16000)\n",
    "        \n",
    "        # 3. Generate emotion embeddings and labels\n",
    "        result = model_audio.generate(\n",
    "            temp_path,\n",
    "            output_dir=\"./outputs\",\n",
    "            granularity=\"utterance\",\n",
    "            extract_embedding=True  # Get both embeddings and emotion scores\n",
    "        )\n",
    "        \n",
    "        if 'scores' in result[0] and 'labels' in result[0]:\n",
    "            # Process emotion labels\n",
    "            scores = result[0]['scores']\n",
    "            labels = result[0]['labels']\n",
    "            english_labels = [label.split('/')[1] if '/' in label else label \n",
    "                            for label in labels]\n",
    "            \n",
    "            # Filter emotions by threshold\n",
    "            emotion_scores = [\n",
    "                (label, score) \n",
    "                for score, label in zip(scores, english_labels)\n",
    "                if score >= threshold\n",
    "            ]\n",
    "            \n",
    "            # Sort by score and take top 3\n",
    "            emotion_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_emotions = emotion_scores[:3]\n",
    "            \n",
    "            # Create emotion sequence string (this is optional I am not doing sequencing for audio atm)\n",
    "            emotion_sequence = \"__\".join([e[0] for e in top_emotions])\n",
    "            \n",
    "            # Store results with embeddings\n",
    "            audio_emotion_sequences.append({\n",
    "                'text': sentence['text'],\n",
    "                'timestamp': sentence['timestamp'],\n",
    "                'emotion_sequence': emotion_sequence, #(this is optional I am not doing sequencing for audio atm)\n",
    "                'emotion_scores': top_emotions,\n",
    "                'embedding': result[0].get('feats', None)  # 1024-dim emotion embedding\n",
    "            })\n",
    "        \n",
    "        # Cleanup\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "            \n",
    "    return audio_emotion_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_sequences = process_audio_emotion_utterances(\n",
    "    sentences=sentences,\n",
    "    audio_file=audio_file_16KhZ,\n",
    "    model_audio=model_audio,\n",
    "    threshold=0.1 # Minimum probability threshold for emotion detection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sentences into emotion sequences\n",
    "text_emo_sequences = process_emotional_flow(sentences)\n",
    "# Process audio segments into emotion sequences\n",
    "audio_emo_sequences = process_audio_emotion_utterances(\n",
    "    sentences=sentences,\n",
    "    audio_file=audio_file_16KhZ,\n",
    "    model_audio=model_audio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_emotion_flow(emotion_embeddings):\n",
    "    # Extract timestamps and emotions\n",
    "    timestamps = [e['timestamp'] for e in emotion_embeddings]\n",
    "    emotion_labels = [e['emotion_scores'][0]['label'] for e in emotion_embeddings]\n",
    "    \n",
    "    # Create time points (midpoint of each sentence)\n",
    "    time_points = [(t[0] + t[1])/2 for t in timestamps]\n",
    "    \n",
    "    # Create a mapping of emotions to colors\n",
    "    unique_emotions = list(set(emotion_labels))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_emotions)))\n",
    "    emotion_colors = {emotion: colors[i] for i, emotion in enumerate(unique_emotions)}\n",
    "    \n",
    "    # Plot emotions over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, (time, emotion) in enumerate(zip(time_points, emotion_labels)):\n",
    "        plt.scatter(time, 1, color=emotion_colors[emotion], s=100)\n",
    "        plt.text(time, 1.05, emotion, rotation=45, ha='center')\n",
    "    \n",
    "    # Add sentence text as annotations\n",
    "    for i, embedding in enumerate(emotion_embeddings):\n",
    "        plt.annotate(embedding['text'][:30] + '...', \n",
    "                     xy=(time_points[i], 0.9),\n",
    "                     xytext=(time_points[i], 0.7),\n",
    "                     arrowprops=dict(arrowstyle='->'),\n",
    "                     ha='center')\n",
    "    \n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.title('Emotion Flow Over Time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_embeddings = generate_emotion_embeddings(text_emo_sequences)\n",
    "\n",
    "emotion_embeddings_MM = generate_emotion_embeddings_MM(text_emo_sequences, audio_emo_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_emotion_analysis(emotion_embeddings_MM):\n",
    "    \"\"\"Display a detailed summary of multimodal emotion analysis\"\"\"\n",
    "    for entry in emotion_embeddings_MM:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Sentence: {entry['text'][:100]}...\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Display timestamps\n",
    "        start, end = entry['timestamp']\n",
    "        print(f\"Time window: {start:.2f}s - {end:.2f}s (duration: {end-start:.2f}s)\")\n",
    "        \n",
    "        # Display emotion sequences\n",
    "        print(\"\\nText Emotions:\", entry['text_emotion_sequence'])\n",
    "        print(\"Audio Emotions:\", entry['audio_emotion_sequence'])\n",
    "        print(\"Combined:\", entry['combined_sequence'])\n",
    "        \n",
    "        # Display top text emotions with scores\n",
    "        print(\"\\nText Emotion Scores:\")\n",
    "        for emotion, score in sorted(entry['text_vector'].items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "            print(f\"  {emotion:<10}: {score:.3f}\")\n",
    "        \n",
    "        # Display top audio emotions with scores\n",
    "        print(\"\\nAudio Emotion Scores:\")\n",
    "        for emotion, score in entry['audio_scores']:\n",
    "            print(f\"  {emotion:<10}: {score:.3f}\")\n",
    "        \n",
    "        # Show embedding dimensionality\n",
    "        print(f\"\\nEmbedding shape: {entry['embedding'].shape}\")\n",
    "\n",
    "# Display the analysis\n",
    "display_emotion_analysis(emotion_embeddings_MM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Temporal Emotion Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_emotion_flow(emotion_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Alternative using multimodal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# Audio: Sharing Embeddings Instead of Raw Recordings\n",
    "Privacy-Preserving Embeddings:\n",
    "Modern research shows that audio embeddings can be designed to retain emotion-relevant features while suppressing speaker identity and other biometric information. Techniques include adversarial training and feature importance-based modification of embeddings, which allow for emotion recognition tasks without exposing speaker identity or reconstructing the original audio signal.\n",
    "\n",
    "Utility and Compliance:\n",
    "Such embeddings are considered privacy-enabled and utility-preserving, supporting emotion detection while reducing risks associated with voiceprints and biometric data. This approach is also compatible with GDPR and similar regulations, as it avoids sharing personally identifiable information.\n",
    "\n",
    "# Text: Anonymization Strategies\n",
    "Automated Anonymization Tools:\n",
    "Use transformer-based models (e.g., BERT, GPT-2) or specialized tools like Microsoft Presidio to detect and redact personal information (names, locations, dates, etc.) from transcripts. These tools can replace sensitive data with placeholders, ensuring the text remains useful for analysis but cannot be traced back to individuals.\n",
    "\n",
    "Manual Review and Metadata Scrubbing:\n",
    "After automated anonymization, manually review transcripts for indirect identifiers (e.g., rare job titles, unique life events) and remove or generalize them as needed. Also, strip or generalize metadata (e.g., device, location, demographics) to further reduce re-identification risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
