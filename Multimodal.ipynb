{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multimodal Emotion recognition system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "**Aim** create a multimodal emotion recognition system based on speach and text\n",
    "\n",
    "I aim to combine: \n",
    "* emotion2vec's audio-based emotion embeddings\n",
    "* text-based emotion embeddings\n",
    "\n",
    "The text-based emotion encoder proposed here treats sentiments as structured entities within an embedding space. Unlike typical classifiers, emotions are not a seen as simple labels extracted by the classifier, the present encorder embeds *emotion sequences* rather than simply extracting a discrete labels. This framework was chosen because it provides a means of encoding latent information underlies the relationships between emotions within flow of text (Liampis, Karanikola, and Kotsiantis [2025](ttps://www.sciencedirect.com/science/article/pii/S0925231225004941)).\n",
    "\n",
    "\n",
    "## Method\n",
    "\n",
    "### Tools\n",
    "\n",
    "* [emotion2vec: audio-based emotion embeddings](https://huggingface.co/emotion2vec/emotion2vec_plus_large)\n",
    "* [emotion-english-distilroberta-base](https://huggingface.co/j-hartmann/emotion-english-roberta-large)\n",
    "* [CrisperWhisper](https://github.com/nyrahealth/CrisperWhisper)\n",
    "\n",
    "**Emotion2vec** is the first universal speech emotion representation model. Through self-supervised pre-training, emotion2vec has the ability to extract emotion representation across different tasks, languages, and scenarios. Emotion2vec aims to create a universal emotional representation space. \n",
    "**Emotion-english-distilroberta-base** model is a distilled version of RoBERTa-large trained on 6 diverse datasets and predicts Ekman's 6 basic emotions, plus a neutral class. RoBERTa-large (Liu et al. 2019) is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. \n",
    "**Crisper-whisper** is an advanced variant of OpenAI's Whisper, designed for fast, precise, and verbatim speech recognition with accurate (crisp) word-level timestamps. Unlike the original Whisper, which tends to omit disfluencies and follows more of a intended transcription style, CrisperWhisper aims to transcribe every spoken word exactly as it is, including fillers, pauses, stutters and false starts.\n",
    "\n",
    "### Data\n",
    "\n",
    "**Input** \n",
    "Audio File: A .wav file containing speech (e.g., audiotest.wav).\n",
    "Transcripts: This analysis uses the crisperwhisper output (JSON format) which provides word-level chunks with timestamps.\n",
    "\n",
    "However, the distributional emotion embedding approach that we want to implement works at the sentence level and therefore we need to also gather sentence level timestamps\n",
    "\n",
    "## Multimodal Processing\n",
    "\n",
    "### Audio embeddings extraction algorythm\n",
    "\n",
    "1. Use emotion2vec to extract audio based emotional embeddings\n",
    "\n",
    "### Transcript generation\n",
    "\n",
    "Transcribe the Audio: Convert the audio file into text using Crisperwhisper.\n",
    "\n",
    "### Text embeddings extraction algorythm\n",
    "\n",
    "**Emotions as distributional entities.** [Liampis 2025](https://www.sciencedirect.com/science/article/pii/S0925231225004941)\n",
    "\n",
    "\n",
    "#### 1. Sentence Tokenization\n",
    "\n",
    "1. segment the transcript into sentences (initially via nltk).\n",
    "2. Tokenize the transcribed text into sentences for emotion analysis\n",
    "3. allocate timestamps to each sentence\n",
    "\n",
    "#### 2. Emotion classification\n",
    "\n",
    "1. Create Emotion Sequences:\n",
    "    1. Apply a multi-label emotion classifier to each sentence in the transcribed text\n",
    "        \n",
    "#### 3. Create Emotion label texts\n",
    "\n",
    "1. Sort extracted emotions by their probability values\n",
    "2. Arrange emotion labels in a serially ordered layout (as strings)\n",
    "   \n",
    "#### 4. Embeddings extraction or generation\n",
    "1. Generate Distributional Representations:\n",
    "    1. Extract embeddings from these emotion sequences using a pre-trained transformer model to extract contextual representations *OR*\n",
    "    2. Combine multiple embedding layers through a weighted average scheme\n",
    "\n",
    "\n",
    "### Multimodal Integration\n",
    "\n",
    "3. Integration with Audio Embeddings (either 1 or 2):\n",
    "    1. Audio-Text Alignment:\n",
    "        1. Map audio segments to corresponding text sentences\n",
    "        2. Extract emotion embeddings from both modalities\n",
    "    2. Cross-Modal Fusion:\n",
    "        1. Concatenate audio and text emotion embeddings     \n",
    "4. Enhanced Classification Framework ([Ni & Ni 2024](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1490796))\n",
    "    1. Emotion Matching: Find the most suitable emotions from an emotion candidate list for the given multimodal input\n",
    "    2. Emotion Correlation Modeling:\n",
    "        1. Implement an attention-based emotion correlation modeling module that learns semantic correlations between emotions from data\n",
    "        2. Obtain correlation-enhanced emotion embedding representations\n",
    "\n",
    "The key innovation of using distributional embeddings is is treating emotion states as tokens in a specialized vocabulary, where each token connects to others through sequential occurrences within text. This allows for capturing the dynamic and sequential nature of emotions as they unfold in content ([Liampis, Karanikola, Kotsiantis 2025](https://www.sciencedirect.com/science/article/pii/S0925231225004941))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from funasr import AutoModel\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## import Audio and txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "audio_file = \"../data/audiotest.wav\"\n",
    "\n",
    "# Load the trancript file in JSON format\n",
    "transcript_file = \"../data/transcripts/audiotest_json.json\"\n",
    "\n",
    "with open(transcript_file, 'r') as file:\n",
    "    transcription_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Audio embeddings extraction\n",
    "\n",
    "### Pre-processing\n",
    "Preprocess audio file to ensure it's in the correct format for emotion2vec (sampling required = 16kHz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def preprocess_audio(audio_file, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Preprocess audio file to ensure it's in the correct format for emotion2vec\n",
    "    \n",
    "    Args:\n",
    "        audio_file (str): Path to the audio file\n",
    "        target_sr (int): Target sampling rate (emotion2vec requires 16kHz)\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the processed audio file\n",
    "    \"\"\"\n",
    "    # Load audio with its original sampling rate\n",
    "    waveform, sr = librosa.load(audio_file, sr=None)\n",
    "    \n",
    "    # Check if resampling is needed\n",
    "    if sr != target_sr:\n",
    "        # Resample to 16kHz\n",
    "        waveform = librosa.resample(waveform, orig_sr=sr, target_sr=target_sr)\n",
    "        \n",
    "        # Save the resampled audio to a temporary file\n",
    "        temp_file = \"temp_resampled.wav\"\n",
    "        sf.write(temp_file, waveform, target_sr)\n",
    "        return temp_file\n",
    "    \n",
    "    return audio_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_file_16KhZ = preprocess_audio(audio_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "[emotion2vec](https://huggingface.co/emotion2vec/emotion2vec_plus_large) is a speech emotion recognition foundation model that can extract emotional features directly from audio waveforms. The emotion2vec_plus_large model is the largest version, trained on 42,526 hours of data. It supports 9 emotion classes:\n",
    "\n",
    "0. angry\n",
    "1. disgusted\n",
    "2. fearful\n",
    "3. happy\n",
    "4. neutral\n",
    "5. other\n",
    "6. sad\n",
    "7. surprised\n",
    "8. unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the emotion2vec_plus_large model\n",
    "from funasr import AutoModel\n",
    "\n",
    "# model=\"iic/emotion2vec_base\"\n",
    "# model=\"iic/emotion2vec_base_finetuned\"\n",
    "# model=\"iic/emotion2vec_plus_seed\"\n",
    "# model=\"iic/emotion2vec_plus_base\" \n",
    "model_id = \"iic/emotion2vec_plus_large\"\n",
    "\n",
    "model_audio = AutoModel(\n",
    "    model=model_id,\n",
    "    hub=\"hf\",  # \"ms\" or \"modelscope\" for China mainland users; \"hf\" or \"huggingface\" for other overseas users\n",
    "    disable_update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    " * *analyze_full_audio()* : Analyze a full audio file to extract both overall emotional tone and the 1024 embedding dimensions for the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a full audio file\n",
    "def analyze_full_audio(audio_file):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(\"./outputs\", exist_ok=True)\n",
    "    \n",
    "    # Generate emotion predictions and extract embeddings\n",
    "    result = model_audio.generate(\n",
    "        audio_file, \n",
    "        output_dir=\"./outputs\", \n",
    "        granularity=\"utterance\", \n",
    "        extract_embedding=True\n",
    "    )\n",
    "    \n",
    "    # Extract embeddings\n",
    "    # result is a list, and each item in the list contains a dictionary with keys like feats, scores, and labels.\n",
    "    if 'feats' in result[0]:\n",
    "        embeddings = result[0]['feats']\n",
    "        print(f\"Embedding dimensionality: {embeddings.shape}\")\n",
    "        return embeddings, result\n",
    "    else:\n",
    "        print(\"No embeddings found in result\")\n",
    "        return None, result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "*analyze_audio_over_time()*: analyze audio over time with sliding windows and hop_length settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze audio over time with sliding windows\n",
    "#\n",
    "# Segment_length (default value of 3 seconds): determines the duration of each \n",
    "#    audio segment that will be analyzed individually.\n",
    "#\n",
    "# Hop_length (default value of 1 second): determines how much the analysis window \n",
    "#    moves forward between consecutive segments. When the hop length is smaller than \n",
    "#    the segment length, consecutive windows overlap.\n",
    "\n",
    "def analyze_audio_over_time(audio_file, segment_length=3, hop_length=1):\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = sf.read(audio_file)\n",
    "    \n",
    "    # Calculate samples\n",
    "    # For example, with a sample rate of 16,000 Hz, a segment length of 3 seconds \n",
    "    # would correspond to 48,000 samples, and a hop length of 1 second would correspond \n",
    "    # to 16,000 samples.\n",
    "    segment_samples = int(segment_length * sample_rate)\n",
    "    hop_samples = int(hop_length * sample_rate)\n",
    "    \n",
    "    # Create temporary segment files and analyze\n",
    "    emotions_over_time = []\n",
    "    timestamps = []\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "    # The function below uses these sample counts to extract overlapping segments from the audio waveform:\n",
    "    # Each segment is saved to a temporary file and analyzed by the emotion recognition model, with the \n",
    "    # results being recorded along with the corresponding timestamp.\n",
    "    for start_idx in range(0, len(waveform) - segment_samples + 1, hop_samples):\n",
    "        \n",
    "        # Extract segment\n",
    "        segment = waveform[start_idx:start_idx + segment_samples]\n",
    "        \n",
    "        # Save temporary segment\n",
    "        temp_path = \"temp_segment.wav\"\n",
    "        sf.write(temp_path, segment, sample_rate)\n",
    "        \n",
    "        # Analyze segment\n",
    "        result = model_audio.generate(\n",
    "            temp_path, \n",
    "            output_dir=\"./outputs\",\n",
    "            granularity=\"utterance\", \n",
    "            extract_embedding=False\n",
    "        )\n",
    "        \n",
    "        # Get emotion scores and labels\n",
    "        if 'scores' in result[0] and 'labels' in result[0]:\n",
    "            scores = result[0]['scores']\n",
    "            labels = result[0]['labels']\n",
    "            \n",
    "            # Record timestamp and scores\n",
    "            timestamp = start_idx / sample_rate\n",
    "            timestamps.append(timestamp)\n",
    "            \n",
    "            # Extract English labels (assuming format \"Chinese/English\")\n",
    "            english_labels = [label.split('/')[1] if '/' in label else label for label in labels]\n",
    "            \n",
    "            # Create a dictionary mapping emotion names to scores\n",
    "            emotion_scores = {}\n",
    "            for i, label in enumerate(english_labels):\n",
    "                emotion_scores[label] = scores[i]\n",
    "            \n",
    "            emotions_over_time.append(emotion_scores)\n",
    "        \n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    df = pd.DataFrame(emotions_over_time, index=timestamps)\n",
    "    \n",
    "    # Fill missing values with 0 (emotions not detected in some segments)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot emotion changes over time\n",
    "def plot_emotions_over_time(df):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for emotion in df.columns:\n",
    "        plt.plot(df.index, df[emotion], label=emotion)\n",
    "    \n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Emotion Probability')\n",
    "    plt.title('Emotion Changes Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze full audio to get embeddings\n",
    "#embeddings, full_result = analyze_full_audio(audio_file_16KhZ)\n",
    "\n",
    "# Analyze audio over time\n",
    "emotion_df = analyze_audio_over_time(audio_file_16KhZ)\n",
    "                                     \n",
    "# Plot emotions over time\n",
    "plot_emotions_over_time(emotion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Text embeddings extraction\n",
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a basic sentence tokenizer - it simply does sentece splitting - no fancy linguistic processing\n",
    "import nltk ## need pip install nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# The JSON output from crisperwhisper contains:\n",
    "#  1. transcribed text without timestamps  as transcription_data[\"text\"]\n",
    "#  2. detailed word-level timestamps as transcription_data[\"chunks\"]\n",
    "def segment_into_sentences(whisper_output):\n",
    "    # Extract full text\n",
    "    full_text = whisper_output['text']\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    \n",
    "    # Map sentences to timestamps using the chunks\n",
    "    sentence_timestamps = []\n",
    "    chunks = whisper_output['chunks']\n",
    "    \n",
    "    current_sentence_idx = 0\n",
    "    current_sentence_words = []\n",
    "    sentence_start_time = chunks[0]['timestamp'][0]\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        word = chunk['text']\n",
    "        timestamp = chunk['timestamp']\n",
    "        \n",
    "        current_sentence_words.append(word)\n",
    "        \n",
    "        # Check if this word ends a sentence\n",
    "        reconstructed = ' '.join(current_sentence_words)\n",
    "        if any(sentences[current_sentence_idx].strip().endswith(reconstructed.strip()) for s in sentences):\n",
    "            sentence_timestamps.append({\n",
    "                'text': sentences[current_sentence_idx],\n",
    "                'timestamp': (sentence_start_time, timestamp[1])\n",
    "            })\n",
    "            \n",
    "            # Move to next sentence\n",
    "            current_sentence_idx += 1\n",
    "            if current_sentence_idx < len(sentences):\n",
    "                current_sentence_words = []\n",
    "                sentence_start_time = timestamp[1]\n",
    "    \n",
    "    return sentence_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = segment_into_sentences(transcription_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Two different representations are needed for our distributional embedding to be constructed. \n",
    "\n",
    "1. Base Corpus Processing:\n",
    "    1. The base corpus (shown on the left) undergoes sentence tokenization ((previous step)\n",
    "    2. Emotion extraction is performed on these sentences using a transformer-based multi-label classifier to \"Basic Emotion Corpus\" containing sentences with their corresponding emotion arrays and probability values\n",
    "      \n",
    "1. Target data processing:\n",
    "    1. In parallel, target data (the specific data you want to model) also undergoes sentence tokenization\n",
    "    2. This creates a \"Target Emotion Corpus\" with emotion tokens* \n",
    "\n",
    "\n",
    "In the previous paragraph we tokenized the base corpus into sentences (via `segment_into_sentences()`). Now, [the paper](https://www.sciencedirect.com/science/article/pii/S0925231225004941) suggests that: \n",
    "\n",
    "1. we use a pre-trained transformer to generate a dataset of labelled sentences *and*\n",
    "2. we assume that the output of the classifier is our ground truth in terms of assigning emotion labels to sentences.\n",
    "\n",
    "According to the paper the Distributional Representations requires to:\n",
    "\n",
    "1. *\"treat each sentence as an observation in a multivariate series of emotions\"* \n",
    "2. *\"transform the emotional flow of a text into a sequence of emotion strings.\"*\n",
    "\n",
    "The textual sequence (our transcript) will have a corresponding emotion-based vector representation\n",
    "\n",
    "We achieve this `classify_emotions()` in 2 steps:\n",
    "\n",
    "### Emotion classification\n",
    "* Step 1: we extract the corresponding emotions for each sentence using the pre-trained transformer ([Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base)) to get the corresponding emotions for each sentence. This approach will lead to having an array of emotions with their probability value. \n",
    "### Create Emotion Label Texts\n",
    "* Step 2: We take the output of `classify_emotions()` and sentence by sentence we interpret the probability value of each emotions in terms of emotion dominance and arrange their corresponding textual labels, i.e., as if they were strings, in serially ordered layout. Very much as if the labels were parts of a text. Therefore, for each sentence in the transcript we find the three most predominant emotions by probability (what the paper calls *\"sort extracted emotions by their probability values\"*). Subsequently, for each sentence `classify_emotions()` generates \"emotion_sequences\" txt by joining (in a serial manner) the top 3 emotions with \"__\" separators. This step implements what [the paper](https://www.sciencedirect.com/science/article/pii/S0925231225004941) calls *\"arrange emotion labels in a serially ordered layout as if the labels were parts of a text. The vocabulary of such a text is the emotion labels.\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch # for GPU use\n",
    "\n",
    "\n",
    "def classify_emotions(sentences, device=None): # Default is None or CPU\n",
    "  # Set device if not provided\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Initialize emotion classifier with device specification\n",
    "    emotion_classifier = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "   #     model=\"j-hartmann/emotion-english-distilroberta-large\",\n",
    "        return_all_scores=True,\n",
    "        device=device  # Use specified GPU (0 for first GPU, 1 for second, etc.)\n",
    "    )\n",
    "    \n",
    "    emotion_sequences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Step 1: we extract the corresponding emotions for each sentence \n",
    "        #   Get emotion probabilities\n",
    "        emotion_scores = emotion_classifier(sentence['text'])[0]\n",
    "\n",
    "        # Step 2: get \"emotion sequences\" (e.g. \"happy_suprised_neutral\"\n",
    "        #   Sort emotions by probability\n",
    "        sorted_emotions = sorted(emotion_scores, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        #   Create emotion sequence (taking top 3 emotions)\n",
    "        top_emotions = sorted_emotions[:3]\n",
    "        emotion_sequence = \"__\".join([e['label'] for e in top_emotions])\n",
    "        \n",
    "        emotion_sequences.append({\n",
    "            'text': sentence['text'],\n",
    "            'timestamp': sentence['timestamp'],\n",
    "            'emotion_sequence': emotion_sequence,\n",
    "            'emotion_scores': sorted_emotions\n",
    "        })\n",
    "    \n",
    "    return emotion_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_sequences = classify_emotions(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "#https://www.sciencedirect.com/science/article/pii/S0925231225004941\n",
    "\n",
    "# the authors use two emotion per sentence, we are using three\n",
    "image_url = \"https://ars.els-cdn.com/content/image/1-s2.0-S0925231225004941-gr2.jpg\"\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "These labels are about the emotion state of each sentence. *\"These emotion states are serial and interdependent and can be treated as novel tokens of a textual sequence consisting only of emotional categorizations - as if the elements in question are, after all, strings\"*\n",
    "\n",
    "[The paper](https://www.sciencedirect.com/science/article/pii/S0925231225004941) specifically mentions that their approach \"treats each sentence as an observation in a multivariate series of emotions\" and transforms \"the emotional flow of a text into a sequence of emotion strings.\" \n",
    "\n",
    "The key innovation proposed is treating emotions as having a \"distributional layout\" in text: the emotion sequences appear and alternate with each other in structured patterns of interrelations. Therefore, after we created emotion sequences that capture the dominant emotions for each sentence in order of probability and we turned them into text we use a transformer model to generate embeddings that capture the contextual relationships between emotions.\n",
    "\n",
    "This method allows to capture both the contextual and sequential dependencies between emotional states as they unfold in the text. *\"Unlike conventional sentiment classification models, which treat emotions as discrete outputs, the methodology we propose encodes sentiments as structured entities within an embedding space.\"*\n",
    "\n",
    "#### Embedding Extraction or Generation\n",
    "After constructing an emotion-state-based corpus (where each sentence has been assigned emotion labels through a multi-label classifier), the process for extracting embeddings involves two alternative approaches:\n",
    "\n",
    "1. **Extraction-based approach**: This uses pre-trained models to extract semantic representations of emotion strings. The paper explains: \"To simply extract the embeddings means that one incorporates a pre-trained scheme to extract the more or less semantic representations of the given emotion strings\". This leverages existing contextual understanding from transformer-based models. *OR*\n",
    "2. **Generation-based approach**: This involves training an embedding scheme from scratch on the serially ordered emotion states. As the paper notes: \"to generate means to train an embedding scheme over the serially ordered emotion states\". This approach captures the unique distributional patterns specific to emotion sequences.\n",
    "\n",
    "\n",
    "Here with `generate_emotion_embeddings()` we use the **extraction-based** approach, where *\"one incorporates a pre-trained scheme* --in this case \"distilber-base-ucased\"--  *to extract the more or less semantic representations of the given emotion strings.\"* \n",
    "\n",
    "1. We uses DistilBERT to capture contextual relationships between emotions in the sequence. The task of the transformer is to extract contextual representations of the emotion sequences (e.g. \"joy__sadness__anger\",\"joy__fear__disgust\"). The transformer self-attention mechanism creates contextual representations where each emotion's embedding is influenced by the other emotions in the sequence. Each self-attention layer of the transformer considers the relationships between the word \"joy\" in the emotion sequence together with the surrounding words influencing how \"joy\" is represented in the vector space. This means the embedding for \"joy\" in \"joy__sadness__anger\" would be different than in \"joy__fear__disgust\" because the surrounding emotions provide different contexts.\n",
    "\n",
    "2. We then create the CLS (**CL**a**S**sification) token embedding as a representation for the entire emotion sequence: we leverage the  pre-trained transformer's ability to aggregate information from the entire emotion sequence into a single vector. As we discussed in the previous paragraph, the model processes the emotion sequence through its attention mechanisms, where \"each attention head can focus on different aspects of meaning\" and \"multiple transformer layers progressively refine the representation\".\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Distributional Representations\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def generate_emotion_embeddings(emotion_sequences):\n",
    "    # Initialize transformer model for contextual representations\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for seq in emotion_sequences:\n",
    "        # Tokenize emotion sequence\n",
    "        inputs = tokenizer(seq['emotion_sequence'], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Use CLS token embedding as sequence representation\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        \n",
    "        seq['embedding'] = embedding\n",
    "        embeddings.append(seq)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_embeddings = generate_emotion_embeddings(emotion_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Temporal Emotion Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_emotion_flow(emotion_embeddings):\n",
    "    # Extract timestamps and emotions\n",
    "    timestamps = [e['timestamp'] for e in emotion_embeddings]\n",
    "    emotion_labels = [e['emotion_scores'][0]['label'] for e in emotion_embeddings]\n",
    "    \n",
    "    # Create time points (midpoint of each sentence)\n",
    "    time_points = [(t[0] + t[1])/2 for t in timestamps]\n",
    "    \n",
    "    # Create a mapping of emotions to colors\n",
    "    unique_emotions = list(set(emotion_labels))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_emotions)))\n",
    "    emotion_colors = {emotion: colors[i] for i, emotion in enumerate(unique_emotions)}\n",
    "    \n",
    "    # Plot emotions over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, (time, emotion) in enumerate(zip(time_points, emotion_labels)):\n",
    "        plt.scatter(time, 1, color=emotion_colors[emotion], s=100)\n",
    "        plt.text(time, 1.05, emotion, rotation=45, ha='center')\n",
    "    \n",
    "    # Add sentence text as annotations\n",
    "    for i, embedding in enumerate(emotion_embeddings):\n",
    "        plt.annotate(embedding['text'][:30] + '...', \n",
    "                     xy=(time_points[i], 0.9),\n",
    "                     xytext=(time_points[i], 0.7),\n",
    "                     arrowprops=dict(arrowstyle='->'),\n",
    "                     ha='center')\n",
    "    \n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.title('Emotion Flow Over Time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_emotion_flow(emotion_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
